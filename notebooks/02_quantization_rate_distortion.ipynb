{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization + Rate-Distortion (Llama Weights)\n",
    "\n",
    "**Objective:** Explore weight-only quantization for Llama models and measure empirical rate-distortion curves vs the Shannon bound.\n",
    "\n",
    "**Bound (Gaussian):**  \n",
    "R(D) = 1/2 * log2(sigma^2 / D) for D <= sigma^2\n",
    "\n",
    "This notebook combines:\n",
    "- Weight extraction (MLP + attention + embeddings)\n",
    "- Quantization sweeps (per-tensor vs per-channel)\n",
    "- Rate-distortion comparisons (uniform, clipped, Lloyd-Max, group)\n",
    "- Vector quantization (k-means, d = 2, 4, 8)\n",
    "- Product quantization (multi-codebook variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUTHENTICATION (required) ===\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token directly as a string argument\n",
    "login(token=\"...\")\n",
    "\n",
    "# After running successfully, DELETE this cell or clear the token string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: Install dependencies ===\n",
    "# Uncomment the lines below to install required packages\n",
    "\n",
    "# Basic dependencies (required)\n",
    "# !pip install -q numpy scipy scikit-learn matplotlib pandas transformers accelerate\n",
    "\n",
    "# For Phase 1: SOTA Quantization Comparison (optional, but recommended)\n",
    "# !pip install -q auto-gptq autoawq bitsandbytes\n",
    "\n",
    "print(\"Dependencies info:\")\n",
    "print(\"  - Basic: numpy, scipy, scikit-learn, matplotlib, pandas, transformers, accelerate\")\n",
    "print(\"  - SOTA (Phase 1): auto-gptq, autoawq, bitsandbytes\")\n",
    "print()\n",
    "print(\"Uncomment the pip install lines above if any packages are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.cluster.vq import kmeans, vq\n",
    "    SCIPY_VQ_OK = True\n",
    "except Exception:\n",
    "    SCIPY_VQ_OK = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (Llama 3.2 1B by default)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weight groups (MLP, attention, embeddings)\n",
    "\n",
    "def extract_weight_groups(model):\n",
    "    mlp = {}\n",
    "    attn = {\"q\": {}, \"k\": {}, \"v\": {}, \"o\": {}}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.endswith(\".weight\"):\n",
    "            continue\n",
    "        if \"model.layers.\" not in name:\n",
    "            continue\n",
    "\n",
    "        parts = name.split(\".\")\n",
    "        try:\n",
    "            layer_num = int(parts[2])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        w = param.detach().cpu().float().numpy()\n",
    "\n",
    "        if \"mlp.down_proj.weight\" in name:\n",
    "            mlp[layer_num] = w\n",
    "        elif \"self_attn.q_proj.weight\" in name:\n",
    "            attn[\"q\"][layer_num] = w\n",
    "        elif \"self_attn.k_proj.weight\" in name:\n",
    "            attn[\"k\"][layer_num] = w\n",
    "        elif \"self_attn.v_proj.weight\" in name:\n",
    "            attn[\"v\"][layer_num] = w\n",
    "        elif \"self_attn.o_proj.weight\" in name:\n",
    "            attn[\"o\"][layer_num] = w\n",
    "\n",
    "    embeddings = {}\n",
    "    try:\n",
    "        embeddings[\"token\"] = model.get_input_embeddings().weight.detach().cpu().float().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if hasattr(model, \"lm_head\") and model.lm_head is not None:\n",
    "        try:\n",
    "            embeddings[\"lm_head\"] = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return mlp, attn, embeddings\n",
    "\n",
    "mlp_weights, attn_weights, embedding_weights = extract_weight_groups(model)\n",
    "\n",
    "print(f\"MLP down-proj layers: {len(mlp_weights)}\")\n",
    "print(\"Attention proj layers:\")\n",
    "for k in [\"q\", \"k\", \"v\", \"o\"]:\n",
    "    print(f\"  {k}: {len(attn_weights[k])}\")\n",
    "print(\"Embeddings:\", list(embedding_weights.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization utilities (per-tensor vs per-channel)\n",
    "\n",
    "def symmetric_quantize(x, bits, per_channel=False, axis=1):\n",
    "    \"\"\"\n",
    "    Symmetric uniform quantization to signed integers.\n",
    "    per_channel=True uses per-output-channel scaling (axis=1 for [out, in]).\n",
    "    Returns dequantized array and scale(s).\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "    if per_channel:\n",
    "        max_abs = np.max(np.abs(x), axis=axis, keepdims=True) + 1e-12\n",
    "        scale = max_abs / qmax\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "    else:\n",
    "        max_abs = float(np.max(np.abs(x)))\n",
    "        scale = max_abs / qmax if max_abs > 0 else 1.0\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "\n",
    "    return dq.astype(np.float32), scale\n",
    "\n",
    "\n",
    "def quant_metrics(x, xq):\n",
    "    x = x.astype(np.float32)\n",
    "    xq = xq.astype(np.float32)\n",
    "    mse = float(np.mean((x - xq) ** 2))\n",
    "    denom = float(np.mean(x ** 2))\n",
    "    sqnr_db = 10 * math.log10(denom / mse) if mse > 0 and denom > 0 else float(\"inf\")\n",
    "    max_abs = float(np.max(np.abs(x)))\n",
    "    max_err = float(np.max(np.abs(x - xq)))\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"sqnr_db\": sqnr_db,\n",
    "        \"max_abs\": max_abs,\n",
    "        \"max_err\": max_err,\n",
    "        \"rel_max_err\": max_err / max_abs if max_abs > 0 else 0.0,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick bit sweep (per-tensor vs per-channel)\n",
    "layer = 8\n",
    "W = mlp_weights[layer]\n",
    "\n",
    "bits_list = [2, 3, 4, 5, 6, 8]\n",
    "\n",
    "results = {\"per_tensor\": [], \"per_channel\": []}\n",
    "\n",
    "for bits in bits_list:\n",
    "    dq_t, _ = symmetric_quantize(W, bits, per_channel=False)\n",
    "    dq_c, _ = symmetric_quantize(W, bits, per_channel=True, axis=1)\n",
    "\n",
    "    results[\"per_tensor\"].append(quant_metrics(W, dq_t))\n",
    "    results[\"per_channel\"].append(quant_metrics(W, dq_c))\n",
    "\n",
    "print(f\"Layer {layer} quantization summary\")\n",
    "print(f\"{'Bits':>6} {'MSE(t)':>12} {'MSE(c)':>12} {'SQNR(t)':>10} {'SQNR(c)':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, bits in enumerate(bits_list):\n",
    "    mt = results[\"per_tensor\"][i]\n",
    "    mc = results[\"per_channel\"][i]\n",
    "    print(f\"{bits:>6} {mt['mse']:>12.2e} {mc['mse']:>12.2e} {mt['sqnr_db']:>10.2f} {mc['sqnr_db']:>10.2f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(bits_list, [r['mse'] for r in results[\"per_tensor\"]], marker='o', label='per-tensor')\n",
    "plt.plot(bits_list, [r['mse'] for r in results[\"per_channel\"]], marker='o', label='per-channel')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Bits')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.title(f'Layer {layer}: Quantization MSE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layerwise sensitivity at a fixed bit-width (per-channel)\n",
    "bits = 4\n",
    "\n",
    "layer_rows = []\n",
    "for layer_num, W in sorted(mlp_weights.items()):\n",
    "    dq, _ = symmetric_quantize(W, bits, per_channel=True, axis=1)\n",
    "    m = quant_metrics(W, dq)\n",
    "    layer_rows.append((layer_num, m[\"mse\"], m[\"sqnr_db\"], m[\"rel_max_err\"]))\n",
    "\n",
    "layer_rows.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print(f\"Top 8 worst MLP layers at {bits}-bit per-channel\")\n",
    "print(f\"{'Layer':>6} {'MSE':>12} {'SQNR(dB)':>10} {'RelMaxErr':>10}\")\n",
    "print(\"-\" * 46)\n",
    "for layer_num, mse, sqnr_db, rel_max_err in layer_rows[:8]:\n",
    "    print(f\"{layer_num:>6} {mse:>12.2e} {sqnr_db:>10.2f} {rel_max_err:>10.3f}\")\n",
    "\n",
    "layers = [r[0] for r in layer_rows]\n",
    "mse_vals = [r[1] for r in layer_rows]\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(layers, mse_vals, marker='o', linewidth=1)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.title(f'MLP down-proj quantization error ({bits}-bit, per-channel)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shannon bound and gap utilities\n",
    "\n",
    "def shannon_distortion(sigma_sq, rate_bits):\n",
    "    return sigma_sq / (4 ** rate_bits)\n",
    "\n",
    "\n",
    "def gap_bits(mse, d_shannon):\n",
    "    if mse <= 0 or d_shannon <= 0:\n",
    "        return 0.0\n",
    "    return 0.5 * math.log2(mse / d_shannon)\n",
    "\n",
    "\n",
    "def interpolate_rate_for_distortion(rates, mses, target_d):\n",
    "    \"\"\"\n",
    "    Estimate R(D) by linear interpolation in log-D space.\n",
    "    rates: list of bits/weight\n",
    "    mses: list of distortions\n",
    "    target_d: target distortion\n",
    "    \"\"\"\n",
    "    rates = np.asarray(rates, dtype=np.float32)\n",
    "    mses = np.asarray(mses, dtype=np.float32)\n",
    "\n",
    "    if target_d <= 0:\n",
    "        return None\n",
    "\n",
    "    order = np.argsort(rates)\n",
    "    rates = rates[order]\n",
    "    mses = mses[order]\n",
    "\n",
    "    logd = np.log(mses + 1e-30)\n",
    "    logt = np.log(target_d)\n",
    "\n",
    "    for i in range(len(rates) - 1):\n",
    "        if (logd[i] >= logt and logd[i + 1] <= logt) or (logd[i] <= logt and logd[i + 1] >= logt):\n",
    "            t = (logt - logd[i]) / (logd[i + 1] - logd[i] + 1e-30)\n",
    "            return float(rates[i] + t * (rates[i + 1] - rates[i]))\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar + group quantization methods\n",
    "\n",
    "def quantize_uniform_asymmetric(weights, bits):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "    w_min, w_max = float(flat.min()), float(flat.max())\n",
    "    if w_max == w_min:\n",
    "        return flat, 0.0\n",
    "    scale = (w_max - w_min) / (levels - 1)\n",
    "    quantized = np.round((flat - w_min) / scale) * scale + w_min\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_uniform_symmetric(weights, bits):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "    max_abs = float(np.max(np.abs(flat)))\n",
    "    if max_abs == 0:\n",
    "        return flat, 0.0\n",
    "    scale = (2 * max_abs) / (levels - 1)\n",
    "    quantized = np.round(flat / scale) * scale\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_symmetric_clipped(weights, bits, clip_sigma=3.0):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "\n",
    "    mu, sigma = float(np.mean(flat)), float(np.std(flat))\n",
    "    clip_val = clip_sigma * sigma\n",
    "    flat_clipped = np.clip(flat, -clip_val, clip_val)\n",
    "\n",
    "    max_abs = float(np.max(np.abs(flat_clipped)))\n",
    "    if max_abs == 0:\n",
    "        return flat, 0.0\n",
    "\n",
    "    scale = (2 * max_abs) / (levels - 1)\n",
    "    quantized = np.round(flat_clipped / scale) * scale\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_group(weights, bits, group_size=128):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (group_size - n % group_size) % group_size\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    groups = flat_padded.reshape(-1, group_size)\n",
    "    levels = 2 ** bits\n",
    "    quantized_groups = np.zeros_like(groups)\n",
    "\n",
    "    for i in range(groups.shape[0]):\n",
    "        group = groups[i]\n",
    "        max_abs = float(np.max(np.abs(group)))\n",
    "        if max_abs == 0:\n",
    "            quantized_groups[i] = group\n",
    "            continue\n",
    "        scale = (2 * max_abs) / (levels - 1)\n",
    "        quantized_groups[i] = np.round(group / scale) * scale\n",
    "\n",
    "    quantized = quantized_groups.flatten()[:n]\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    effective_bits = bits + 16 / group_size  # FP16 scale per group\n",
    "    return quantized, mse, effective_bits\n",
    "\n",
    "\n",
    "def quantize_lloyd_max(weights, bits, max_iter=20):  # OPTIMIZED: reduced from 100 to 20\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "\n",
    "    percentiles = np.linspace(0, 100, levels + 2)[1:-1]\n",
    "    centroids = np.percentile(flat, percentiles)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        boundaries = (centroids[:-1] + centroids[1:]) / 2\n",
    "        boundaries = np.concatenate([[-np.inf], boundaries, [np.inf]])\n",
    "        assignments = np.digitize(flat, boundaries[1:-1])\n",
    "\n",
    "        new_centroids = np.zeros(levels, dtype=np.float32)\n",
    "        for i in range(levels):\n",
    "            mask = assignments == i\n",
    "            if np.any(mask):\n",
    "                new_centroids[i] = np.mean(flat[mask])\n",
    "            else:\n",
    "                new_centroids[i] = centroids[i]\n",
    "\n",
    "        if np.allclose(centroids, new_centroids, rtol=1e-6):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    boundaries = (centroids[:-1] + centroids[1:]) / 2\n",
    "    assignments = np.digitize(flat, boundaries)\n",
    "    assignments = np.clip(assignments, 0, levels - 1)\n",
    "    quantized = centroids[assignments]\n",
    "\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means helpers (vector + product quantization)\n",
    "\n",
    "def _kmeans_fit(vectors, n_centroids, seed=0):\n",
    "    if SKLEARN_OK:\n",
    "        km = MiniBatchKMeans(\n",
    "            n_clusters=n_centroids,\n",
    "            random_state=seed,\n",
    "            batch_size=4096,\n",
    "            n_init=3,\n",
    "            max_iter=50,  # OPTIMIZED: reduced from 100 to 50\n",
    "        )\n",
    "        km.fit(vectors)\n",
    "        return km\n",
    "\n",
    "    if SCIPY_VQ_OK:\n",
    "        centroids, _ = kmeans(vectors, n_centroids, iter=20)\n",
    "        return centroids\n",
    "\n",
    "    raise RuntimeError(\"No k-means backend found. Install scikit-learn or scipy.\")\n",
    "\n",
    "\n",
    "def _kmeans_assign(vectors, model_or_centroids):\n",
    "    if SKLEARN_OK and hasattr(model_or_centroids, \"predict\"):\n",
    "        return model_or_centroids.predict(vectors), model_or_centroids.cluster_centers_\n",
    "\n",
    "    if SCIPY_VQ_OK:\n",
    "        assignments, _ = vq(vectors, model_or_centroids)\n",
    "        return assignments, model_or_centroids\n",
    "\n",
    "    raise RuntimeError(\"No k-means backend found. Install scikit-learn or scipy.\")\n",
    "\n",
    "\n",
    "def quantize_kmeans_vq(weights, bits, dim=2, sample_vectors=10000, eval_vectors=10000, seed=0):  # OPTIMIZED: reduced from 20000 to 10000\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (dim - n % dim) % dim\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    vectors = flat_padded.reshape(-1, dim)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if sample_vectors is not None and len(vectors) > sample_vectors:\n",
    "        idx = rng.choice(len(vectors), size=sample_vectors, replace=False)\n",
    "        train_vectors = vectors[idx]\n",
    "    else:\n",
    "        train_vectors = vectors\n",
    "\n",
    "    n_centroids = 2 ** bits\n",
    "    model = _kmeans_fit(train_vectors, n_centroids, seed=seed)\n",
    "\n",
    "    if eval_vectors is not None and len(vectors) > eval_vectors:\n",
    "        idx = rng.choice(len(vectors), size=eval_vectors, replace=False)\n",
    "        eval_vectors_subset = vectors[idx]\n",
    "    else:\n",
    "        eval_vectors_subset = vectors\n",
    "\n",
    "    assignments, centroids = _kmeans_assign(eval_vectors_subset, model)\n",
    "    quantized_vectors = centroids[assignments]\n",
    "\n",
    "    mse = float(np.mean((eval_vectors_subset - quantized_vectors) ** 2))\n",
    "    effective_bits = bits / dim\n",
    "    return quantized_vectors, mse, effective_bits\n",
    "\n",
    "\n",
    "def quantize_product_quantization(weights, bits, dim=4, codebooks=4, sample_vectors=10000, eval_vectors=10000, seed=0):  # OPTIMIZED: reduced from 20000 to 10000\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (dim - n % dim) % dim\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    vectors = flat_padded.reshape(-1, dim)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    group_ids = np.arange(len(vectors)) % codebooks\n",
    "\n",
    "    total_se = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for g in range(codebooks):\n",
    "        group_vecs = vectors[group_ids == g]\n",
    "        if len(group_vecs) == 0:\n",
    "            continue\n",
    "\n",
    "        if sample_vectors is not None and len(group_vecs) > sample_vectors:\n",
    "            idx = rng.choice(len(group_vecs), size=sample_vectors, replace=False)\n",
    "            train_vecs = group_vecs[idx]\n",
    "        else:\n",
    "            train_vecs = group_vecs\n",
    "\n",
    "        n_centroids = 2 ** bits\n",
    "        model = _kmeans_fit(train_vecs, n_centroids, seed=seed + g)\n",
    "\n",
    "        if eval_vectors is not None and len(group_vecs) > eval_vectors:\n",
    "            idx = rng.choice(len(group_vecs), size=eval_vectors, replace=False)\n",
    "            eval_vecs = group_vecs[idx]\n",
    "        else:\n",
    "            eval_vecs = group_vecs\n",
    "\n",
    "        assignments, centroids = _kmeans_assign(eval_vecs, model)\n",
    "        quantized = centroids[assignments]\n",
    "\n",
    "        total_se += float(np.sum((eval_vecs - quantized) ** 2))\n",
    "        total_n += eval_vecs.size\n",
    "\n",
    "    mse = total_se / total_n if total_n > 0 else 0.0\n",
    "    effective_bits = bits / dim\n",
    "    return mse, effective_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar quantizers: rate-distortion curves\n",
    "\n",
    "def measure_scalar_quantizers(weights, bits_list=range(1, 9)):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    sigma_sq = float(np.var(flat))\n",
    "\n",
    "    results = {\n",
    "        \"bits\": [],\n",
    "        \"shannon_d\": [],\n",
    "        \"uniform_asym\": [],\n",
    "        \"uniform_sym\": [],\n",
    "        \"clipped_3sigma\": [],\n",
    "        \"group_128\": [],\n",
    "        \"group_32\": [],\n",
    "        \"lloyd_max\": [],\n",
    "        \"group_128_rate\": [],\n",
    "        \"group_32_rate\": [],\n",
    "    }\n",
    "\n",
    "    for bits in bits_list:\n",
    "        print(f\"  Computing {bits}-bit quantization...\")\n",
    "        d_shannon = shannon_distortion(sigma_sq, bits)\n",
    "\n",
    "        results[\"bits\"].append(bits)\n",
    "        results[\"shannon_d\"].append(d_shannon)\n",
    "\n",
    "        _, mse = quantize_uniform_asymmetric(weights, bits)\n",
    "        results[\"uniform_asym\"].append(mse)\n",
    "\n",
    "        _, mse = quantize_uniform_symmetric(weights, bits)\n",
    "        results[\"uniform_sym\"].append(mse)\n",
    "\n",
    "        _, mse = quantize_symmetric_clipped(weights, bits, clip_sigma=3.0)\n",
    "        results[\"clipped_3sigma\"].append(mse)\n",
    "\n",
    "        _, mse, eff = quantize_group(weights, bits, group_size=128)\n",
    "        results[\"group_128\"].append(mse)\n",
    "        results[\"group_128_rate\"].append(eff)\n",
    "\n",
    "        _, mse, eff = quantize_group(weights, bits, group_size=32)\n",
    "        results[\"group_32\"].append(mse)\n",
    "        results[\"group_32_rate\"].append(eff)\n",
    "\n",
    "        _, mse = quantize_lloyd_max(weights, bits)\n",
    "        results[\"lloyd_max\"].append(mse)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_rate_distortion_scalar(results, title=\"Rate-Distortion (Scalar and Group)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bits = np.array(results[\"bits\"], dtype=np.float32)\n",
    "    plt.semilogy(bits, results[\"shannon_d\"], 'k--', linewidth=2, marker='o', label='Shannon bound')\n",
    "\n",
    "    methods = [\n",
    "        (\"uniform_asym\", \"Uniform (naive)\", 'red'),\n",
    "        (\"uniform_sym\", \"Uniform (symmetric)\", 'orange'),\n",
    "        (\"clipped_3sigma\", \"Clipped 3sigma\", 'green'),\n",
    "        (\"lloyd_max\", \"Lloyd-Max\", 'cyan'),\n",
    "    ]\n",
    "\n",
    "    for key, label, color in methods:\n",
    "        plt.semilogy(bits, results[key], '-', linewidth=1.5, marker='s', label=label, color=color)\n",
    "\n",
    "    plt.semilogy(results[\"group_128_rate\"], results[\"group_128\"], '-', linewidth=1.5, marker='^', label='Group g=128', color='blue')\n",
    "    plt.semilogy(results[\"group_32_rate\"], results[\"group_32\"], '-', linewidth=1.5, marker='^', label='Group g=32', color='purple')\n",
    "\n",
    "    plt.xlabel('Rate (bits per weight)')\n",
    "    plt.ylabel('Distortion (MSE)')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector and product quantization helpers\n",
    "\n",
    "def measure_vector_quantizers(weights, dims=(2, 4, 8), codebook_bits=6):  # OPTIMIZED: default reduced from 8 to 6\n",
    "    curves = []\n",
    "    for d in dims:\n",
    "        print(f\"  VQ: dim={d}, codebook_bits={codebook_bits}\")\n",
    "        _, mse, eff_bits = quantize_kmeans_vq(\n",
    "            weights,\n",
    "            bits=codebook_bits,\n",
    "            dim=d,\n",
    "            sample_vectors=10000,  # OPTIMIZED: reduced from 20000\n",
    "            eval_vectors=10000,    # OPTIMIZED: reduced from 20000\n",
    "            seed=0,\n",
    "        )\n",
    "        curves.append({\n",
    "            \"label\": f\"VQ d={d} (codebook {codebook_bits} bits)\",\n",
    "            \"rates\": [eff_bits],\n",
    "            \"mses\": [mse],\n",
    "        })\n",
    "    return curves\n",
    "\n",
    "\n",
    "def measure_product_quantizers(weights, dims=(2, 4, 8), codebook_bits=6, codebooks=4):  # OPTIMIZED: default reduced from 8 to 6\n",
    "    curves = []\n",
    "    for d in dims:\n",
    "        print(f\"  PQ: dim={d}, codebook_bits={codebook_bits}, codebooks={codebooks}\")\n",
    "        mse, eff_bits = quantize_product_quantization(\n",
    "            weights,\n",
    "            bits=codebook_bits,\n",
    "            dim=d,\n",
    "            codebooks=codebooks,\n",
    "            sample_vectors=10000,  # OPTIMIZED: reduced from 20000\n",
    "            eval_vectors=10000,    # OPTIMIZED: reduced from 20000\n",
    "            seed=0,\n",
    "        )\n",
    "        curves.append({\n",
    "            \"label\": f\"PQ d={d} (codebook {codebook_bits} bits, {codebooks} codebooks)\",\n",
    "            \"rates\": [eff_bits],\n",
    "            \"mses\": [mse],\n",
    "        })\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate-distortion analysis for a single layer\n",
    "layer = 8\n",
    "weights = mlp_weights[layer]\n",
    "\n",
    "print(f\"Analyzing MLP layer {layer}\")\n",
    "print(f\"Shape: {weights.shape}, Total weights: {weights.size:,}\")\n",
    "print(f\"Variance: {np.var(weights.astype(np.float32)):.6e}\")\n",
    "\n",
    "# OPTIMIZED: reduced bits_list from range(1,9) to key values [2,3,4,5,6,8]\n",
    "scalar_results = measure_scalar_quantizers(weights, bits_list=[2, 3, 4, 5, 6, 8])\n",
    "plot_rate_distortion_scalar(scalar_results, title=f\"Rate-Distortion: MLP Layer {layer}\")\n",
    "\n",
    "# OPTIMIZED: reduced codebook_bits from 8 to 6 for faster k-means\n",
    "vq_curves = measure_vector_quantizers(weights, dims=(2, 4, 8), codebook_bits=6)\n",
    "pq_curves = measure_product_quantizers(weights, dims=(2, 4, 8), codebook_bits=6, codebooks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay vector and product quantization points on scalar curves\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bits = np.array(scalar_results[\"bits\"], dtype=np.float32)\n",
    "plt.semilogy(bits, scalar_results[\"shannon_d\"], 'k--', linewidth=2, marker='o', label='Shannon bound')\n",
    "\n",
    "for key, label, color in [\n",
    "    (\"uniform_sym\", \"Uniform (symmetric)\", 'orange'),\n",
    "    (\"clipped_3sigma\", \"Clipped 3sigma\", 'green'),\n",
    "    (\"lloyd_max\", \"Lloyd-Max\", 'cyan'),\n",
    "]:\n",
    "    plt.semilogy(bits, scalar_results[key], '-', linewidth=1.5, marker='s', label=label, color=color)\n",
    "\n",
    "plt.semilogy(scalar_results[\"group_128_rate\"], scalar_results[\"group_128\"], '-', linewidth=1.5, marker='^', label='Group g=128', color='blue')\n",
    "plt.semilogy(scalar_results[\"group_32_rate\"], scalar_results[\"group_32\"], '-', linewidth=1.5, marker='^', label='Group g=32', color='purple')\n",
    "\n",
    "for curve in vq_curves + pq_curves:\n",
    "    plt.semilogy(curve[\"rates\"], curve[\"mses\"], 'D', markersize=7, label=curve[\"label\"])\n",
    "\n",
    "plt.xlabel('Rate (bits per weight)')\n",
    "plt.ylabel('Distortion (MSE)')\n",
    "plt.title(f\"Rate-Distortion (MLP layer {layer})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap at target distortions\n",
    "\n",
    "sigma_sq = float(np.var(weights.flatten().astype(np.float32)))\n",
    "\n",
    "targets = [0.001 * sigma_sq, 0.01 * sigma_sq]\n",
    "\n",
    "print(\"Target distortions:\")\n",
    "for t in targets:\n",
    "    print(f\"  D = {t:.3e}\")\n",
    "\n",
    "curves = [\n",
    "    (\"Uniform (symmetric)\", scalar_results[\"bits\"], scalar_results[\"uniform_sym\"]),\n",
    "    (\"Clipped 3sigma\", scalar_results[\"bits\"], scalar_results[\"clipped_3sigma\"]),\n",
    "    (\"Group g=128\", scalar_results[\"group_128_rate\"], scalar_results[\"group_128\"]),\n",
    "    (\"Group g=32\", scalar_results[\"group_32_rate\"], scalar_results[\"group_32\"]),\n",
    "    (\"Lloyd-Max\", scalar_results[\"bits\"], scalar_results[\"lloyd_max\"]),\n",
    "]\n",
    "\n",
    "for t in targets:\n",
    "    print(f\"\n",
    "=== Gap summary at D={t:.3e} ===\")\n",
    "    for label, rates, mses in curves:\n",
    "        r_emp = interpolate_rate_for_distortion(rates, mses, t)\n",
    "        if r_emp is None:\n",
    "            print(f\"  {label:<16} : insufficient coverage\")\n",
    "            continue\n",
    "        r_shannon = 0.5 * math.log2(sigma_sq / t)\n",
    "        gap = r_emp - r_shannon\n",
    "        print(f\"  {label:<16} : R_emp={r_emp:.2f} bits, gap={gap:.2f} bits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-group comparison (MLP vs Attention vs Embeddings)\n",
    "\n",
    "bits_list = list(range(1, 9))\n",
    "\n",
    "\n",
    "def select_layers(layer_dict, max_layers=6):\n",
    "    layers = sorted(layer_dict.keys())\n",
    "    if len(layers) <= max_layers:\n",
    "        return layers\n",
    "    idx = np.linspace(0, len(layers) - 1, max_layers).round().astype(int)\n",
    "    return [layers[i] for i in idx]\n",
    "\n",
    "\n",
    "def avg_rd_curve(weights_list, bits_list, quantizer_fn):\n",
    "    mses = []\n",
    "    for bits in bits_list:\n",
    "        layer_mses = []\n",
    "        for W in weights_list:\n",
    "            _, mse = quantizer_fn(W, bits)\n",
    "            layer_mses.append(mse)\n",
    "        mses.append(float(np.mean(layer_mses)))\n",
    "    return mses\n",
    "\n",
    "mlp_layers = select_layers(mlp_weights, max_layers=6)\n",
    "q_layers = select_layers(attn_weights[\"q\"], max_layers=6)\n",
    "\n",
    "mlp_list = [mlp_weights[i] for i in mlp_layers]\n",
    "q_list = [attn_weights[\"q\"][i] for i in q_layers]\n",
    "\n",
    "mlp_curve = avg_rd_curve(mlp_list, bits_list, quantize_uniform_symmetric)\n",
    "q_curve = avg_rd_curve(q_list, bits_list, quantize_uniform_symmetric)\n",
    "\n",
    "emb_curve = None\n",
    "if \"token\" in embedding_weights:\n",
    "    emb_curve = [quantize_uniform_symmetric(embedding_weights[\"token\"], b)[1] for b in bits_list]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(bits_list, mlp_curve, '-o', label=f\"MLP down (avg {len(mlp_layers)} layers)\")\n",
    "plt.semilogy(bits_list, q_curve, '-o', label=f\"Attention Q (avg {len(q_layers)} layers)\")\n",
    "if emb_curve is not None:\n",
    "    plt.semilogy(bits_list, emb_curve, '-o', label=\"Token embeddings\")\n",
    "\n",
    "plt.xlabel('Rate (bits per weight)')\n",
    "plt.ylabel('Distortion (MSE)')\n",
    "plt.title('Per-group comparison (uniform symmetric)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation guide\n",
    "\n",
    "**Gap in bits** (empirical vs Shannon):\n",
    "- 0 bits gap: unattainable ideal (Shannon bound)\n",
    "- 0.25 bits: theoretical minimum for uniform scalar on Gaussian\n",
    "- 0.5 to 1.0 bits: strong practical quantization\n",
    "- 1.0 to 2.0 bits: meaningful headroom\n",
    "\n",
    "**If Lloyd-Max is still far from bound:** vector quantization or structured models (e.g., lattices or PQ) are needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: State-of-the-Art Comparison\n",
    "\n",
    "**Objective:** Compare our simple quantization methods against production-grade GPTQ, AWQ, and bitsandbytes to understand the gap.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How much better is GPTQ/AWQ (with Hessian-based error compensation) than simple uniform quantization?\n",
    "2. Does NF4 (Gaussian-optimized) beat FP4 (uniform log-spacing)? This validates our Gaussian assumption.\n",
    "3. Is weight MSE a good proxy for functional distortion (output MSE)?\n",
    "\n",
    "**Expected findings:**\n",
    "- GPTQ/AWQ should achieve ~0.5-1.0 bits better than our simple methods due to error compensation\n",
    "- NF4 should beat FP4, confirming near-Gaussian weight distributions\n",
    "- Weight importance should be non-uniform (functional MSE != weight MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Load Pre-Quantized GPTQ/AWQ Models\n",
    "\n",
    "Install required libraries and load quantized checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load Original FP16 Model (already loaded above)\n",
    "# ============================================\n",
    "# We'll reuse the 'model' variable from cell 4 as our FP16 baseline\n",
    "\n",
    "# ============================================\n",
    "# Load AWQ 4-bit Quantized Model\n",
    "# ============================================\n",
    "print(\"Loading AWQ 4-bit model...\")\n",
    "try:\n",
    "    from transformers import AwqConfig\n",
    "    \n",
    "    awq_config = AwqConfig(\n",
    "        bits=4,\n",
    "        fuse_max_seq_len=512,\n",
    "        do_fuse=False  # Disable kernel fusion for weight extraction\n",
    "    )\n",
    "    \n",
    "    model_awq = AutoModelForCausalLM.from_pretrained(\n",
    "        \"AMead10/Llama-3.2-1B-Instruct-AWQ\",\n",
    "        quantization_config=awq_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ AWQ model loaded successfully\")\n",
    "    AWQ_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ AWQ model loading failed: {e}\")\n",
    "    print(\"  Install with: pip install autoawq\")\n",
    "    model_awq = None\n",
    "    AWQ_AVAILABLE = False\n",
    "\n",
    "# ============================================\n",
    "# Load GPTQ 4-bit Quantized Model\n",
    "# ============================================\n",
    "print(\"\\nLoading GPTQ 4-bit model...\")\n",
    "try:\n",
    "    model_gptq = AutoModelForCausalLM.from_pretrained(\n",
    "        \"clowman/Llama-3.2-1B-Instruct-GPTQ-Int4\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ GPTQ model loaded successfully\")\n",
    "    GPTQ_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ GPTQ model loading failed: {e}\")\n",
    "    print(\"  Install with: pip install auto-gptq\")\n",
    "    model_gptq = None\n",
    "    GPTQ_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nModel availability summary:\")\n",
    "print(f\"  FP16 baseline: ✓ (already loaded)\")\n",
    "print(f\"  AWQ 4-bit: {'✓' if AWQ_AVAILABLE else '✗'}\")\n",
    "print(f\"  GPTQ 4-bit: {'✓' if GPTQ_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare Weights Across Models\n",
    "# ============================================\n",
    "\n",
    "# Target layer for comparison (same as before)\n",
    "target_layer = 8\n",
    "target_proj = \"mlp.down_proj\"  # or try \"mlp.up_proj\", \"mlp.gate_proj\", \"self_attn.q_proj\", etc.\n",
    "\n",
    "# Extract FP16 baseline weights (convert to fp32 for comparison)\n",
    "w_fp16 = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Analyzing layer {target_layer}, projection: {target_proj}\")\n",
    "print(f\"Weight shape: {w_fp16.shape}\")\n",
    "print(f\"FP16 variance: {np.var(w_fp16):.6e}\")\n",
    "print(f\"FP16 kurtosis: {np.mean(((w_fp16 - w_fp16.mean()) / w_fp16.std())**4):.2f}\")\n",
    "print()\n",
    "\n",
    "# Container for results\n",
    "sota_results = {\n",
    "    \"method\": [\"FP16 (baseline)\"],\n",
    "    \"mse\": [0.0],\n",
    "    \"bits_per_weight\": [16.0],\n",
    "    \"sqnr_db\": [float('inf')],\n",
    "}\n",
    "\n",
    "# Extract AWQ weights if available\n",
    "if AWQ_AVAILABLE and model_awq is not None:\n",
    "    try:\n",
    "        w_awq = model_awq.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "        mse_awq = np.mean((w_fp16 - w_awq) ** 2)\n",
    "        sqnr_awq = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_awq) if mse_awq > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"AWQ 4-bit\")\n",
    "        sota_results[\"mse\"].append(mse_awq)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_awq)\n",
    "        \n",
    "        print(f\"AWQ 4-bit:\")\n",
    "        print(f\"  MSE:     {mse_awq:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_awq:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_awq)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract AWQ weights: {e}\\n\")\n",
    "\n",
    "# Extract GPTQ weights if available\n",
    "if GPTQ_AVAILABLE and model_gptq is not None:\n",
    "    try:\n",
    "        w_gptq = model_gptq.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "        mse_gptq = np.mean((w_fp16 - w_gptq) ** 2)\n",
    "        sqnr_gptq = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_gptq) if mse_gptq > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"GPTQ 4-bit\")\n",
    "        sota_results[\"mse\"].append(mse_gptq)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_gptq)\n",
    "        \n",
    "        print(f\"GPTQ 4-bit:\")\n",
    "        print(f\"  MSE:     {mse_gptq:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_gptq:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_gptq)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract GPTQ weights: {e}\\n\")\n",
    "\n",
    "# Add our simple methods for comparison (4-bit)\n",
    "print(\"Comparing to our simple quantization methods:\")\n",
    "\n",
    "# Per-tensor symmetric 4-bit\n",
    "dq_per_tensor, _ = symmetric_quantize(w_fp16, 4, per_channel=False)\n",
    "mse_per_tensor = np.mean((w_fp16 - dq_per_tensor) ** 2)\n",
    "sqnr_per_tensor = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_tensor)\n",
    "sota_results[\"method\"].append(\"Simple per-tensor\")\n",
    "sota_results[\"mse\"].append(mse_per_tensor)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_tensor)\n",
    "print(f\"  Simple per-tensor 4-bit: MSE={mse_per_tensor:.2e}, SQNR={sqnr_per_tensor:.2f} dB\")\n",
    "\n",
    "# Per-channel symmetric 4-bit\n",
    "dq_per_channel, _ = symmetric_quantize(w_fp16, 4, per_channel=True, axis=1)\n",
    "mse_per_channel = np.mean((w_fp16 - dq_per_channel) ** 2)\n",
    "sqnr_per_channel = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_channel)\n",
    "sota_results[\"method\"].append(\"Simple per-channel\")\n",
    "sota_results[\"mse\"].append(mse_per_channel)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_channel)\n",
    "print(f\"  Simple per-channel 4-bit: MSE={mse_per_channel:.2e}, SQNR={sqnr_per_channel:.2f} dB\")\n",
    "\n",
    "# Group quantization (g=128)\n",
    "_, mse_group128, eff_bits_g128 = quantize_group(w_fp16, 4, group_size=128)\n",
    "sqnr_group128 = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_group128)\n",
    "sota_results[\"method\"].append(\"Group g=128\")\n",
    "sota_results[\"mse\"].append(mse_group128)\n",
    "sota_results[\"bits_per_weight\"].append(eff_bits_g128)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_group128)\n",
    "print(f\"  Group g=128 (4-bit): MSE={mse_group128:.2e}, SQNR={sqnr_group128:.2f} dB, eff_bits={eff_bits_g128:.3f}\")\n",
    "\n",
    "# Shannon bound (Gaussian assumption)\n",
    "sigma_sq = np.var(w_fp16)\n",
    "d_shannon_4bit = shannon_distortion(sigma_sq, 4.0)\n",
    "sqnr_shannon = 10 * np.log10(np.mean(w_fp16 ** 2) / d_shannon_4bit)\n",
    "sota_results[\"method\"].append(\"Shannon bound (4-bit)\")\n",
    "sota_results[\"mse\"].append(d_shannon_4bit)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "print(f\"  Shannon bound (4-bit): MSE={d_shannon_4bit:.2e}, SQNR={sqnr_shannon:.2f} dB\")\n",
    "print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "import pandas as pd\n",
    "df_sota = pd.DataFrame(sota_results)\n",
    "df_sota = df_sota.sort_values(\"mse\", ascending=True)\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE (sorted by MSE, lower is better)\")\n",
    "print(\"=\" * 70)\n",
    "print(df_sota.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize SOTA Comparison\n",
    "# ============================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MSE comparison (bar chart)\n",
    "methods_plot = [m for m in df_sota[\"method\"] if m != \"FP16 (baseline)\"]\n",
    "mses_plot = [df_sota[df_sota[\"method\"] == m][\"mse\"].values[0] for m in methods_plot]\n",
    "\n",
    "colors = []\n",
    "for m in methods_plot:\n",
    "    if \"Shannon\" in m:\n",
    "        colors.append('black')\n",
    "    elif \"AWQ\" in m or \"GPTQ\" in m:\n",
    "        colors.append('green')\n",
    "    elif \"Group\" in m:\n",
    "        colors.append('blue')\n",
    "    else:\n",
    "        colors.append('orange')\n",
    "\n",
    "ax1.bar(range(len(methods_plot)), mses_plot, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(methods_plot)))\n",
    "ax1.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel('MSE (log scale)')\n",
    "ax1.set_title(f'Weight MSE Comparison (Layer {target_layer})')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: SQNR comparison (higher is better)\n",
    "sqnrs_plot = [df_sota[df_sota[\"method\"] == m][\"sqnr_db\"].values[0] for m in methods_plot]\n",
    "sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs_plot]  # Cap infinity at 100 for plotting\n",
    "\n",
    "ax2.bar(range(len(methods_plot)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods_plot)))\n",
    "ax2.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax2.set_ylabel('SQNR (dB)')\n",
    "ax2.set_title(f'Signal-to-Quantization-Noise Ratio (Layer {target_layer})')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=sqnr_shannon, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Shannon bound')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display gaps\n",
    "print(\"\\nGap analysis (bits from Shannon bound at 4-bit):\")\n",
    "print(\"=\" * 60)\n",
    "for method in methods_plot:\n",
    "    if \"Shannon\" in method:\n",
    "        continue\n",
    "    row = df_sota[df_sota[\"method\"] == method].iloc[0]\n",
    "    mse = row[\"mse\"]\n",
    "    gap = gap_bits(mse, d_shannon_4bit)\n",
    "    print(f\"{method:25s}: {gap:+.3f} bits gap\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Compare bitsandbytes NF4 vs FP4\n",
    "\n",
    "**Hypothesis:** If weights are near-Gaussian, NF4 (optimized for Gaussian) should outperform FP4 (uniform log-spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load with NF4 (NormalFloat4 - optimized for Gaussian)\n",
    "# ============================================\n",
    "print(\"Loading NF4 (Gaussian-optimized) quantized model...\")\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    config_nf4 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False  # Disable nested quantization for cleaner comparison\n",
    "    )\n",
    "    \n",
    "    model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        quantization_config=config_nf4,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"✓ NF4 model loaded successfully\")\n",
    "    NF4_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ NF4 loading failed: {e}\")\n",
    "    print(\"  Install with: pip install bitsandbytes\")\n",
    "    model_nf4 = None\n",
    "    NF4_AVAILABLE = False\n",
    "\n",
    "# ============================================\n",
    "# Load with FP4 (uniform log-spacing)\n",
    "# ============================================\n",
    "print(\"\\nLoading FP4 (uniform log-spacing) quantized model...\")\n",
    "try:\n",
    "    config_fp4 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"fp4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False\n",
    "    )\n",
    "    \n",
    "    model_fp4 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        quantization_config=config_fp4,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"✓ FP4 model loaded successfully\")\n",
    "    FP4_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ FP4 loading failed: {e}\")\n",
    "    model_fp4 = None\n",
    "    FP4_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nbitsandbytes availability:\")\n",
    "print(f\"  NF4 (Gaussian-optimized): {'✓' if NF4_AVAILABLE else '✗'}\")\n",
    "print(f\"  FP4 (uniform log):        {'✓' if FP4_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare NF4 vs FP4 Weights\n",
    "# ============================================\n",
    "\n",
    "# Note: bitsandbytes stores weights in quantized format with block-wise scaling.\n",
    "# We need to trigger dequantization by accessing the weight directly.\n",
    "\n",
    "bnb_results = {\n",
    "    \"method\": [],\n",
    "    \"mse\": [],\n",
    "    \"sqnr_db\": [],\n",
    "}\n",
    "\n",
    "# We already have FP16 baseline from earlier\n",
    "w_fp16_ref = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Comparing NF4 vs FP4 on layer {target_layer}, {target_proj}\")\n",
    "print(f\"FP16 reference shape: {w_fp16_ref.shape}\")\n",
    "print()\n",
    "\n",
    "# Extract NF4 weights\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    try:\n",
    "        # Access the quantized weight - this triggers dequantization\n",
    "        w_nf4_tensor = model_nf4.model.layers[target_layer].mlp.down_proj.weight\n",
    "        # Force computation to trigger dequantization\n",
    "        w_nf4 = w_nf4_tensor.detach().cpu().float().numpy()\n",
    "        \n",
    "        mse_nf4 = np.mean((w_fp16_ref - w_nf4) ** 2)\n",
    "        sqnr_nf4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_nf4) if mse_nf4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"NF4 (Gaussian-optimized)\")\n",
    "        bnb_results[\"mse\"].append(mse_nf4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_nf4)\n",
    "        \n",
    "        print(f\"NF4 (Gaussian-optimized):\")\n",
    "        print(f\"  MSE:     {mse_nf4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_nf4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_nf4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract NF4 weights: {e}\\n\")\n",
    "\n",
    "# Extract FP4 weights\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    try:\n",
    "        w_fp4_tensor = model_fp4.model.layers[target_layer].mlp.down_proj.weight\n",
    "        w_fp4 = w_fp4_tensor.detach().cpu().float().numpy()\n",
    "        \n",
    "        mse_fp4 = np.mean((w_fp16_ref - w_fp4) ** 2)\n",
    "        sqnr_fp4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_fp4) if mse_fp4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"FP4 (uniform log)\")\n",
    "        bnb_results[\"mse\"].append(mse_fp4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_fp4)\n",
    "        \n",
    "        print(f\"FP4 (uniform log-spacing):\")\n",
    "        print(f\"  MSE:     {mse_fp4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_fp4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_fp4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract FP4 weights: {e}\\n\")\n",
    "\n",
    "# Add Shannon bound for reference\n",
    "sigma_sq = np.var(w_fp16_ref)\n",
    "d_shannon_4bit = shannon_distortion(sigma_sq, 4.0)\n",
    "sqnr_shannon = 10 * np.log10(np.mean(w_fp16_ref ** 2) / d_shannon_4bit)\n",
    "\n",
    "bnb_results[\"method\"].append(\"Shannon bound\")\n",
    "bnb_results[\"mse\"].append(d_shannon_4bit)\n",
    "bnb_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "\n",
    "# Display comparison\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    df_bnb = pd.DataFrame(bnb_results)\n",
    "    df_bnb = df_bnb.sort_values(\"mse\", ascending=True)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NF4 vs FP4 COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_bnb.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Interpretation\n",
    "    if NF4_AVAILABLE and FP4_AVAILABLE:\n",
    "        improvement = (mse_fp4 - mse_nf4) / mse_fp4 * 100 if mse_fp4 > 0 else 0\n",
    "        print(f\"\\nNF4 achieves {improvement:.1f}% lower MSE than FP4\")\n",
    "        if improvement > 5:\n",
    "            print(\"✓ VALIDATION: NF4 significantly outperforms FP4\")\n",
    "            print(\"  → Confirms weights are approximately Gaussian\")\n",
    "        else:\n",
    "            print(\"⚠ UNEXPECTED: NF4 and FP4 perform similarly\")\n",
    "            print(\"  → Weights may be more heavy-tailed than expected\")\n",
    "else:\n",
    "    print(\"⚠ Could not load bitsandbytes models for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize NF4 vs FP4 Comparison\n",
    "# ============================================\n",
    "\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    methods = df_bnb[\"method\"].tolist()\n",
    "    mses = df_bnb[\"mse\"].tolist()\n",
    "    sqnrs = df_bnb[\"sqnr_db\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'NF4' in m else 'orange' if 'FP4' in m else 'black' for m in methods]\n",
    "    \n",
    "    # MSE comparison\n",
    "    ax1.bar(range(len(methods)), mses, color=colors, alpha=0.7)\n",
    "    ax1.set_xticks(range(len(methods)))\n",
    "    ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel('MSE (log scale)')\n",
    "    ax1.set_title('NF4 vs FP4: Weight MSE')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # SQNR comparison\n",
    "    sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs]\n",
    "    ax2.bar(range(len(methods)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('SQNR (dB)')\n",
    "    ax2.set_title('NF4 vs FP4: Signal Quality')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gap analysis\n",
    "    print(\"\\nGap from Shannon bound:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, mse in zip(methods, mses):\n",
    "        if \"Shannon\" not in method:\n",
    "            gap = gap_bits(mse, d_shannon_4bit)\n",
    "            print(f\"{method:25s}: {gap:+.3f} bits\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"Visualization skipped - bitsandbytes models not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Measure Functional Distortion\n",
    "\n",
    "**Key Question:** Is weight MSE a good proxy for output distortion?\n",
    "\n",
    "AWQ's insight: Some weights matter more than others. If weight importance is non-uniform, we should see that weight MSE doesn't correlate perfectly with functional MSE (output logits error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Prepare Calibration Data\n",
    "# ============================================\n",
    "\n",
    "print(\"Preparing calibration dataset for functional distortion measurement...\")\n",
    "\n",
    "# Simple calibration texts (you can expand this with more diverse samples)\n",
    "calibration_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The weather today is sunny with a chance of rain.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Transformers have revolutionized natural language processing.\",\n",
    "    \"Climate change is one of the biggest challenges facing humanity.\",\n",
    "    \"The stock market experienced volatility in recent months.\",\n",
    "    \"Quantum computing promises to solve complex problems faster.\",\n",
    "    \"Renewable energy sources include solar, wind, and hydroelectric power.\",\n",
    "    \"The human brain contains approximately 86 billion neurons.\",\n",
    "]\n",
    "\n",
    "# Tokenize calibration texts\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "calibration_inputs = tokenizer(\n",
    "    calibration_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"Calibration dataset: {len(calibration_texts)} samples\")\n",
    "print(f\"Input shape: {calibration_inputs['input_ids'].shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Compute Functional Distortion (Logits MSE)\n",
    "# ============================================\n",
    "\n",
    "def compute_functional_mse(model1, model2, inputs, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute MSE between output logits of two models.\n",
    "    \"\"\"\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs2 = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits1 = outputs1.logits.cpu().float()\n",
    "        logits2 = outputs2.logits.cpu().float()\n",
    "        \n",
    "        # Compute MSE over all logits\n",
    "        functional_mse = torch.mean((logits1 - logits2) ** 2).item()\n",
    "        \n",
    "        # Also compute per-token MSE for analysis\n",
    "        per_token_mse = torch.mean((logits1 - logits2) ** 2, dim=(0, 2)).numpy()\n",
    "        \n",
    "    return functional_mse, per_token_mse\n",
    "\n",
    "\n",
    "# Determine device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "# Container for results\n",
    "functional_results = {\n",
    "    \"method\": [],\n",
    "    \"weight_mse\": [],\n",
    "    \"functional_mse\": [],\n",
    "    \"ratio\": [],  # functional_mse / weight_mse\n",
    "}\n",
    "\n",
    "# Baseline: FP16 vs FP16 (should be ~0)\n",
    "print(\"Computing functional distortion...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Move models to device if needed\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# For each quantized model, compute both weight and functional MSE\n",
    "models_to_compare = []\n",
    "\n",
    "if AWQ_AVAILABLE and model_awq is not None:\n",
    "    models_to_compare.append((\"AWQ 4-bit\", model_awq, mse_awq if 'mse_awq' in locals() else None))\n",
    "\n",
    "if GPTQ_AVAILABLE and model_gptq is not None:\n",
    "    models_to_compare.append((\"GPTQ 4-bit\", model_gptq, mse_gptq if 'mse_gptq' in locals() else None))\n",
    "\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    models_to_compare.append((\"NF4 4-bit\", model_nf4, mse_nf4 if 'mse_nf4' in locals() else None))\n",
    "\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    models_to_compare.append((\"FP4 4-bit\", model_fp4, mse_fp4 if 'mse_fp4' in locals() else None))\n",
    "\n",
    "# Compute functional MSE for each model\n",
    "for method_name, quant_model, weight_mse in models_to_compare:\n",
    "    try:\n",
    "        print(f\"Computing functional MSE for {method_name}...\")\n",
    "        func_mse, per_token = compute_functional_mse(model, quant_model, calibration_inputs, device=device)\n",
    "        \n",
    "        functional_results[\"method\"].append(method_name)\n",
    "        functional_results[\"weight_mse\"].append(weight_mse if weight_mse is not None else np.nan)\n",
    "        functional_results[\"functional_mse\"].append(func_mse)\n",
    "        functional_results[\"ratio\"].append(func_mse / weight_mse if weight_mse and weight_mse > 0 else np.nan)\n",
    "        \n",
    "        print(f\"  Weight MSE:     {weight_mse:.2e}\" if weight_mse else \"  Weight MSE:     N/A\")\n",
    "        print(f\"  Functional MSE: {func_mse:.2e}\")\n",
    "        print(f\"  Ratio (F/W):    {func_mse / weight_mse:.2e}\" if weight_mse and weight_mse > 0 else \"  Ratio (F/W):    N/A\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    df_functional = pd.DataFrame(functional_results)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FUNCTIONAL DISTORTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_functional.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"⚠ No models available for functional distortion comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Weight MSE vs Functional MSE\n",
    "# ============================================\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Weight MSE vs Functional MSE (scatter)\n",
    "    valid_mask = ~np.isnan(df_functional[\"weight_mse\"]) & ~np.isnan(df_functional[\"functional_mse\"])\n",
    "    df_valid = df_functional[valid_mask]\n",
    "    \n",
    "    if len(df_valid) > 0:\n",
    "        ax1.scatter(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"], s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in df_valid.iterrows():\n",
    "            ax1.annotate(row[\"method\"], \n",
    "                        (row[\"weight_mse\"], row[\"functional_mse\"]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        ax1.set_xlabel('Weight MSE (log scale)')\n",
    "        ax1.set_ylabel('Functional MSE (log scale)')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title('Weight MSE vs Functional MSE')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add diagonal reference line (y=x would mean perfect correlation)\n",
    "        xlim = ax1.get_xlim()\n",
    "        ylim = ax1.get_ylim()\n",
    "        min_val = max(xlim[0], ylim[0])\n",
    "        max_val = min(xlim[1], ylim[1])\n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3, linewidth=1, label='y=x')\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Plot 2: Bar chart of functional MSE\n",
    "    methods = df_functional[\"method\"].tolist()\n",
    "    func_mses = df_functional[\"functional_mse\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'AWQ' in m or 'GPTQ' in m else 'blue' if 'NF4' in m else 'orange' for m in methods]\n",
    "    \n",
    "    ax2.bar(range(len(methods)), func_mses, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Functional MSE (log scale)')\n",
    "    ax2.set_title('Output Logits Error by Method')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis: Correlation between weight and functional MSE\n",
    "    if len(df_valid) > 1:\n",
    "        from scipy.stats import spearmanr, pearsonr\n",
    "        \n",
    "        corr_pearson, p_pearson = pearsonr(np.log(df_valid[\"weight_mse\"]), np.log(df_valid[\"functional_mse\"]))\n",
    "        corr_spearman, p_spearman = spearmanr(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"])\n",
    "        \n",
    "        print(\"\\nCorrelation Analysis:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Pearson correlation (log-log):  r={corr_pearson:.3f}, p={p_pearson:.4f}\")\n",
    "        print(f\"Spearman correlation (rank):    ρ={corr_spearman:.3f}, p={p_spearman:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if corr_spearman > 0.8:\n",
    "            print(\"✓ Strong correlation: Weight MSE is a good proxy for functional MSE\")\n",
    "        elif corr_spearman > 0.5:\n",
    "            print(\"⚠ Moderate correlation: Weight MSE partially predicts functional MSE\")\n",
    "        else:\n",
    "            print(\"✗ Weak correlation: Weight importance is highly non-uniform\")\n",
    "            print(\"  → AWQ's per-channel importance weighting is crucial\")\n",
    "else:\n",
    "    print(\"Visualization skipped - no functional distortion data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Findings from SOTA Comparison\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "1. **GPTQ/AWQ vs Simple Methods:**\n",
    "   - GPTQ/AWQ should achieve 0.5-1.0 bits better compression at same distortion\n",
    "   - This gap comes from Hessian-based error compensation\n",
    "\n",
    "2. **NF4 vs FP4:**\n",
    "   - If NF4 >> FP4: Confirms weights are approximately Gaussian\n",
    "   - If NF4 ≈ FP4: Weights may be heavier-tailed than expected\n",
    "\n",
    "3. **Weight MSE vs Functional MSE:**\n",
    "   - Strong correlation (ρ > 0.8): Weight MSE is a good proxy, uniform importance\n",
    "   - Weak correlation (ρ < 0.5): Weight importance is non-uniform, per-channel schemes critical\n",
    "\n",
    "**Research Opportunities:**\n",
    "- If GPTQ/AWQ gap is large: Implement Hessian-based importance weighting\n",
    "- If NF4 wins significantly: Explore optimal quantization levels for near-Gaussian distributions\n",
    "- If functional MSE deviates: Investigate per-layer and per-channel importance patterns"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "02_quantization_rate_distortion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05eefd6d0d1c47f3864dfb2d5e951baa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "23b93d9eae154fe9920d11d5b255b232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_47df1d8880e14f8e83b66a0288ebd212",
      "style": "IPY_MODEL_a2a22f647ee348f3afb8911967911d76",
      "value": false
     }
    },
    "27f4ba733c4a4898be92c44c4b605059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_05eefd6d0d1c47f3864dfb2d5e951baa"
     }
    },
    "2f84979196f14096bb40470e9754e1cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cfc628fb819416c8ed1d25529dbe1a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_52366869bc994a68839231af9aea5ca5",
      "style": "IPY_MODEL_76244519e573454aa20e57353f538156",
      "tooltip": ""
     }
    },
    "42fef59f00de4c6d91a54a842e92df8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_bf349272eb844eda9d0ab12ace710dfb",
      "placeholder": "​",
      "style": "IPY_MODEL_a4eaea336a9446238b1d0c83f1cd1110",
      "value": ""
     }
    },
    "47df1d8880e14f8e83b66a0288ebd212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52366869bc994a68839231af9aea5ca5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "585f59894a524e6b80e1b4c913b7f354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65e0bd7115334929a41c7250b2dd92fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76244519e573454aa20e57353f538156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "8d9bc2b198104ef3907b1fc84c94b52f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9447651252ec4ec4a1498a62cc340145": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98a4dc51322449efa9a498f4b0c40c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2a22f647ee348f3afb8911967911d76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4eaea336a9446238b1d0c83f1cd1110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf349272eb844eda9d0ab12ace710dfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2f7a3bd06e74d31bbbd0230c92d7db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9447651252ec4ec4a1498a62cc340145",
      "placeholder": "​",
      "style": "IPY_MODEL_2f84979196f14096bb40470e9754e1cc",
      "value": "Connecting..."
     }
    },
    "ce024e6f79624ea285bc033f0b5e7488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65e0bd7115334929a41c7250b2dd92fa",
      "placeholder": "​",
      "style": "IPY_MODEL_98a4dc51322449efa9a498f4b0c40c81",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e7d920fc5f8144098c886b0478df5ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_585f59894a524e6b80e1b4c913b7f354",
      "placeholder": "​",
      "style": "IPY_MODEL_8d9bc2b198104ef3907b1fc84c94b52f",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
