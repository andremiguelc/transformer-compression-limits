{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Quantization + Rate-Distortion: Trivial Methods\n",
    "\n",
    "**Objective:** Explore simple weight-only quantization methods for Llama models and measure empirical rate-distortion curves vs the Shannon bound.\n",
    "\n",
    "**Shannon Bound (Gaussian):**  \n",
    "R(D) = 1/2 * log2(sigma^2 / D) for D <= sigma^2\n",
    "\n",
    "This notebook covers:\n",
    "- Weight extraction (MLP + attention + embeddings)\n",
    "- Simple quantization sweeps (per-tensor vs per-channel)\n",
    "- Rate-distortion comparisons (uniform, clipped, Lloyd-Max, group)\n",
    "- Vector quantization (k-means, d = 2, 4, 8)\n",
    "- Product quantization (multi-codebook variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUTHENTICATION (required) ===\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token directly as a string argument\n",
    "login(token=\"...\")\n",
    "\n",
    "# After running successfully, DELETE this cell or clear the token string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Install dependencies ===\n",
    "# Uncomment the lines below to install required packages\n",
    "\n",
    "%pip install -q torch torchvision\n",
    "%pip install -q numpy scipy scikit-learn matplotlib pandas transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "# Initial Quantization Experiments\n",
    "I did this just to get a sense of how to quantize weights in transformers for trivial methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.cluster.vq import kmeans, vq\n",
    "    SCIPY_VQ_OK = True\n",
    "except Exception:\n",
    "    SCIPY_VQ_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (Llama 3.2 1B by default)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weight groups (MLP, attention, embeddings)\n",
    "\n",
    "def extract_weight_groups(model):\n",
    "    mlp = {}\n",
    "    attn = {\"q\": {}, \"k\": {}, \"v\": {}, \"o\": {}}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.endswith(\".weight\"):\n",
    "            continue\n",
    "        if \"model.layers.\" not in name:\n",
    "            continue\n",
    "\n",
    "        parts = name.split(\".\")\n",
    "        try:\n",
    "            layer_num = int(parts[2])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        w = param.detach().cpu().float().numpy()\n",
    "\n",
    "        if \"mlp.down_proj.weight\" in name:\n",
    "            mlp[layer_num] = w\n",
    "        elif \"self_attn.q_proj.weight\" in name:\n",
    "            attn[\"q\"][layer_num] = w\n",
    "        elif \"self_attn.k_proj.weight\" in name:\n",
    "            attn[\"k\"][layer_num] = w\n",
    "        elif \"self_attn.v_proj.weight\" in name:\n",
    "            attn[\"v\"][layer_num] = w\n",
    "        elif \"self_attn.o_proj.weight\" in name:\n",
    "            attn[\"o\"][layer_num] = w\n",
    "\n",
    "    embeddings = {}\n",
    "    try:\n",
    "        embeddings[\"token\"] = model.get_input_embeddings().weight.detach().cpu().float().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if hasattr(model, \"lm_head\") and model.lm_head is not None:\n",
    "        try:\n",
    "            embeddings[\"lm_head\"] = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return mlp, attn, embeddings\n",
    "\n",
    "mlp_weights, attn_weights, embedding_weights = extract_weight_groups(model)\n",
    "\n",
    "print(f\"MLP down-proj layers: {len(mlp_weights)}\")\n",
    "print(\"Attention proj layers:\")\n",
    "for k in [\"q\", \"k\", \"v\", \"o\"]:\n",
    "    print(f\"  {k}: {len(attn_weights[k])}\")\n",
    "print(\"Embeddings:\", list(embedding_weights.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization utilities (per-tensor vs per-channel)\n",
    "\n",
    "def symmetric_quantize(x, bits, per_channel=False, axis=1):\n",
    "    \"\"\"\n",
    "    Symmetric uniform quantization to signed integers.\n",
    "    per_channel=True uses per-output-channel scaling (axis=1 for [out, in]).\n",
    "    Returns dequantized array and scale(s).\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "    if per_channel:\n",
    "        max_abs = np.max(np.abs(x), axis=axis, keepdims=True) + 1e-12\n",
    "        scale = max_abs / qmax\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "    else:\n",
    "        max_abs = float(np.max(np.abs(x)))\n",
    "        scale = max_abs / qmax if max_abs > 0 else 1.0\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "\n",
    "    return dq.astype(np.float32), scale\n",
    "\n",
    "\n",
    "def quant_metrics(x, xq):\n",
    "    x = x.astype(np.float32)\n",
    "    xq = xq.astype(np.float32)\n",
    "    mse = float(np.mean((x - xq) ** 2))\n",
    "    denom = float(np.mean(x ** 2))\n",
    "    sqnr_db = 10 * math.log10(denom / mse) if mse > 0 and denom > 0 else float(\"inf\")\n",
    "    max_abs = float(np.max(np.abs(x)))\n",
    "    max_err = float(np.max(np.abs(x - xq)))\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"sqnr_db\": sqnr_db,\n",
    "        \"max_abs\": max_abs,\n",
    "        \"max_err\": max_err,\n",
    "        \"rel_max_err\": max_err / max_abs if max_abs > 0 else 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick bit sweep (per-tensor vs per-channel)\n",
    "layer = 8\n",
    "W = mlp_weights[layer]\n",
    "\n",
    "bits_list = [2, 3, 4, 5, 6, 8]\n",
    "\n",
    "results = {\"per_tensor\": [], \"per_channel\": []}\n",
    "\n",
    "for bits in bits_list:\n",
    "    dq_t, _ = symmetric_quantize(W, bits, per_channel=False)\n",
    "    dq_c, _ = symmetric_quantize(W, bits, per_channel=True, axis=1)\n",
    "\n",
    "    results[\"per_tensor\"].append(quant_metrics(W, dq_t))\n",
    "    results[\"per_channel\"].append(quant_metrics(W, dq_c))\n",
    "\n",
    "print(f\"Layer {layer} quantization summary\")\n",
    "print(f\"{'Bits':>6} {'MSE(t)':>12} {'MSE(c)':>12} {'SQNR(t)':>10} {'SQNR(c)':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, bits in enumerate(bits_list):\n",
    "    mt = results[\"per_tensor\"][i]\n",
    "    mc = results[\"per_channel\"][i]\n",
    "    print(f\"{bits:>6} {mt['mse']:>12.2e} {mc['mse']:>12.2e} {mt['sqnr_db']:>10.2f} {mc['sqnr_db']:>10.2f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(bits_list, [r['mse'] for r in results[\"per_tensor\"]], marker='o', label='per-tensor')\n",
    "plt.plot(bits_list, [r['mse'] for r in results[\"per_channel\"]], marker='o', label='per-channel')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Bits')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.title(f'Layer {layer}: Quantization MSE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layerwise sensitivity at a fixed bit-width (per-channel)\n",
    "bits = 4\n",
    "\n",
    "layer_rows = []\n",
    "for layer_num, W in sorted(mlp_weights.items()):\n",
    "    dq, _ = symmetric_quantize(W, bits, per_channel=True, axis=1)\n",
    "    m = quant_metrics(W, dq)\n",
    "    layer_rows.append((layer_num, m[\"mse\"], m[\"sqnr_db\"], m[\"rel_max_err\"]))\n",
    "\n",
    "layer_rows.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print(f\"Top 8 worst MLP layers at {bits}-bit per-channel\")\n",
    "print(f\"{'Layer':>6} {'MSE':>12} {'SQNR(dB)':>10} {'RelMaxErr':>10}\")\n",
    "print(\"-\" * 46)\n",
    "for layer_num, mse, sqnr_db, rel_max_err in layer_rows[:8]:\n",
    "    print(f\"{layer_num:>6} {mse:>12.2e} {sqnr_db:>10.2f} {rel_max_err:>10.3f}\")\n",
    "\n",
    "layers = [r[0] for r in layer_rows]\n",
    "mse_vals = [r[1] for r in layer_rows]\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(layers, mse_vals, marker='o', linewidth=1)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.title(f'MLP down-proj quantization error ({bits}-bit, per-channel)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shannon bound and gap utilities\n",
    "\n",
    "def shannon_distortion(sigma_sq, rate_bits):\n",
    "    return sigma_sq / (4 ** rate_bits)\n",
    "\n",
    "\n",
    "def gap_bits(mse, d_shannon):\n",
    "    if mse <= 0 or d_shannon <= 0:\n",
    "        return 0.0\n",
    "    return 0.5 * math.log2(mse / d_shannon)\n",
    "\n",
    "\n",
    "def interpolate_rate_for_distortion(rates, mses, target_d):\n",
    "    \"\"\"\n",
    "    Estimate R(D) by linear interpolation in log-D space.\n",
    "    rates: list of bits/weight\n",
    "    mses: list of distortions\n",
    "    target_d: target distortion\n",
    "    \"\"\"\n",
    "    rates = np.asarray(rates, dtype=np.float32)\n",
    "    mses = np.asarray(mses, dtype=np.float32)\n",
    "\n",
    "    if target_d <= 0:\n",
    "        return None\n",
    "\n",
    "    order = np.argsort(rates)\n",
    "    rates = rates[order]\n",
    "    mses = mses[order]\n",
    "\n",
    "    logd = np.log(mses + 1e-30)\n",
    "    logt = np.log(target_d)\n",
    "\n",
    "    for i in range(len(rates) - 1):\n",
    "        if (logd[i] >= logt and logd[i + 1] <= logt) or (logd[i] <= logt and logd[i + 1] >= logt):\n",
    "            t = (logt - logd[i]) / (logd[i + 1] - logd[i] + 1e-30)\n",
    "            return float(rates[i] + t * (rates[i + 1] - rates[i]))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar + group quantization methods\n",
    "\n",
    "def quantize_uniform_asymmetric(weights, bits):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "    w_min, w_max = float(flat.min()), float(flat.max())\n",
    "    if w_max == w_min:\n",
    "        return flat, 0.0\n",
    "    scale = (w_max - w_min) / (levels - 1)\n",
    "    quantized = np.round((flat - w_min) / scale) * scale + w_min\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_uniform_symmetric(weights, bits):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "    max_abs = float(np.max(np.abs(flat)))\n",
    "    if max_abs == 0:\n",
    "        return flat, 0.0\n",
    "    scale = (2 * max_abs) / (levels - 1)\n",
    "    quantized = np.round(flat / scale) * scale\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_symmetric_clipped(weights, bits, clip_sigma=3.0):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "\n",
    "    mu, sigma = float(np.mean(flat)), float(np.std(flat))\n",
    "    clip_val = clip_sigma * sigma\n",
    "    flat_clipped = np.clip(flat, -clip_val, clip_val)\n",
    "\n",
    "    max_abs = float(np.max(np.abs(flat_clipped)))\n",
    "    if max_abs == 0:\n",
    "        return flat, 0.0\n",
    "\n",
    "    scale = (2 * max_abs) / (levels - 1)\n",
    "    quantized = np.round(flat_clipped / scale) * scale\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse\n",
    "\n",
    "\n",
    "def quantize_group(weights, bits, group_size=128):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (group_size - n % group_size) % group_size\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    groups = flat_padded.reshape(-1, group_size)\n",
    "    levels = 2 ** bits\n",
    "    quantized_groups = np.zeros_like(groups)\n",
    "\n",
    "    for i in range(groups.shape[0]):\n",
    "        group = groups[i]\n",
    "        max_abs = float(np.max(np.abs(group)))\n",
    "        if max_abs == 0:\n",
    "            quantized_groups[i] = group\n",
    "            continue\n",
    "        scale = (2 * max_abs) / (levels - 1)\n",
    "        quantized_groups[i] = np.round(group / scale) * scale\n",
    "\n",
    "    quantized = quantized_groups.flatten()[:n]\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    effective_bits = bits + 16 / group_size  # FP16 scale per group\n",
    "    return quantized, mse, effective_bits\n",
    "\n",
    "\n",
    "def quantize_lloyd_max(weights, bits, max_iter=20):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    levels = 2 ** bits\n",
    "\n",
    "    percentiles = np.linspace(0, 100, levels + 2)[1:-1]\n",
    "    centroids = np.percentile(flat, percentiles)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        boundaries = (centroids[:-1] + centroids[1:]) / 2\n",
    "        boundaries = np.concatenate([[-np.inf], boundaries, [np.inf]])\n",
    "        assignments = np.digitize(flat, boundaries[1:-1])\n",
    "\n",
    "        new_centroids = np.zeros(levels, dtype=np.float32)\n",
    "        for i in range(levels):\n",
    "            mask = assignments == i\n",
    "            if np.any(mask):\n",
    "                new_centroids[i] = np.mean(flat[mask])\n",
    "            else:\n",
    "                new_centroids[i] = centroids[i]\n",
    "\n",
    "        if np.allclose(centroids, new_centroids, rtol=1e-6):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    boundaries = (centroids[:-1] + centroids[1:]) / 2\n",
    "    assignments = np.digitize(flat, boundaries)\n",
    "    assignments = np.clip(assignments, 0, levels - 1)\n",
    "    quantized = centroids[assignments]\n",
    "\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    return quantized, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means helpers (vector + product quantization)\n",
    "\n",
    "def _kmeans_fit(vectors, n_centroids, seed=0):\n",
    "    if SKLEARN_OK:\n",
    "        km = MiniBatchKMeans(\n",
    "            n_clusters=n_centroids,\n",
    "            random_state=seed,\n",
    "            batch_size=4096,\n",
    "            n_init=3,\n",
    "            max_iter=50,\n",
    "        )\n",
    "        km.fit(vectors)\n",
    "        return km\n",
    "\n",
    "    if SCIPY_VQ_OK:\n",
    "        centroids, _ = kmeans(vectors, n_centroids, iter=20)\n",
    "        return centroids\n",
    "\n",
    "    raise RuntimeError(\"No k-means backend found. Install scikit-learn or scipy.\")\n",
    "\n",
    "\n",
    "def _kmeans_assign(vectors, model_or_centroids):\n",
    "    if SKLEARN_OK and hasattr(model_or_centroids, \"predict\"):\n",
    "        return model_or_centroids.predict(vectors), model_or_centroids.cluster_centers_\n",
    "\n",
    "    if SCIPY_VQ_OK:\n",
    "        assignments, _ = vq(vectors, model_or_centroids)\n",
    "        return assignments, model_or_centroids\n",
    "\n",
    "    raise RuntimeError(\"No k-means backend found. Install scikit-learn or scipy.\")\n",
    "\n",
    "\n",
    "def quantize_kmeans_vq(weights, bits, dim=2, sample_vectors=10000, eval_vectors=10000, seed=0):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (dim - n % dim) % dim\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    vectors = flat_padded.reshape(-1, dim)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if sample_vectors is not None and len(vectors) > sample_vectors:\n",
    "        idx = rng.choice(len(vectors), size=sample_vectors, replace=False)\n",
    "        train_vectors = vectors[idx]\n",
    "    else:\n",
    "        train_vectors = vectors\n",
    "\n",
    "    n_centroids = 2 ** bits\n",
    "    model = _kmeans_fit(train_vectors, n_centroids, seed=seed)\n",
    "\n",
    "    if eval_vectors is not None and len(vectors) > eval_vectors:\n",
    "        idx = rng.choice(len(vectors), size=eval_vectors, replace=False)\n",
    "        eval_vectors_subset = vectors[idx]\n",
    "    else:\n",
    "        eval_vectors_subset = vectors\n",
    "\n",
    "    assignments, centroids = _kmeans_assign(eval_vectors_subset, model)\n",
    "    quantized_vectors = centroids[assignments]\n",
    "\n",
    "    mse = float(np.mean((eval_vectors_subset - quantized_vectors) ** 2))\n",
    "    effective_bits = bits / dim\n",
    "    return quantized_vectors, mse, effective_bits\n",
    "\n",
    "\n",
    "def quantize_product_quantization(weights, bits, dim=4, codebooks=4, sample_vectors=10000, eval_vectors=10000, seed=0):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (dim - n % dim) % dim\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    vectors = flat_padded.reshape(-1, dim)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    group_ids = np.arange(len(vectors)) % codebooks\n",
    "\n",
    "    total_se = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for g in range(codebooks):\n",
    "        group_vecs = vectors[group_ids == g]\n",
    "        if len(group_vecs) == 0:\n",
    "            continue\n",
    "\n",
    "        if sample_vectors is not None and len(group_vecs) > sample_vectors:\n",
    "            idx = rng.choice(len(group_vecs), size=sample_vectors, replace=False)\n",
    "            train_vecs = group_vecs[idx]\n",
    "        else:\n",
    "            train_vecs = group_vecs\n",
    "\n",
    "        n_centroids = 2 ** bits\n",
    "        model = _kmeans_fit(train_vecs, n_centroids, seed=seed + g)\n",
    "\n",
    "        if eval_vectors is not None and len(group_vecs) > eval_vectors:\n",
    "            idx = rng.choice(len(group_vecs), size=eval_vectors, replace=False)\n",
    "            eval_vecs = group_vecs[idx]\n",
    "        else:\n",
    "            eval_vecs = group_vecs\n",
    "\n",
    "        assignments, centroids = _kmeans_assign(eval_vecs, model)\n",
    "        quantized = centroids[assignments]\n",
    "\n",
    "        total_se += float(np.sum((eval_vecs - quantized) ** 2))\n",
    "        total_n += eval_vecs.size\n",
    "\n",
    "    mse = total_se / total_n if total_n > 0 else 0.0\n",
    "    effective_bits = bits / dim\n",
    "    return mse, effective_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar quantizers: rate-distortion curves\n",
    "\n",
    "def measure_scalar_quantizers(weights, bits_list=range(1, 9)):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    sigma_sq = float(np.var(flat))\n",
    "\n",
    "    results = {\n",
    "        \"bits\": [],\n",
    "        \"shannon_d\": [],\n",
    "        \"uniform_asym\": [],\n",
    "        \"uniform_sym\": [],\n",
    "        \"clipped_3sigma\": [],\n",
    "        \"group_128\": [],\n",
    "        \"group_32\": [],\n",
    "        \"lloyd_max\": [],\n",
    "        \"group_128_rate\": [],\n",
    "        \"group_32_rate\": [],\n",
    "    }\n",
    "\n",
    "    for bits in bits_list:\n",
    "        print(f\"  Computing {bits}-bit quantization...\")\n",
    "        d_shannon = shannon_distortion(sigma_sq, bits)\n",
    "\n",
    "        results[\"bits\"].append(bits)\n",
    "        results[\"shannon_d\"].append(d_shannon)\n",
    "\n",
    "        _, mse = quantize_uniform_asymmetric(weights, bits)\n",
    "        results[\"uniform_asym\"].append(mse)\n",
    "\n",
    "        _, mse = quantize_uniform_symmetric(weights, bits)\n",
    "        results[\"uniform_sym\"].append(mse)\n",
    "\n",
    "        _, mse = quantize_symmetric_clipped(weights, bits, clip_sigma=3.0)\n",
    "        results[\"clipped_3sigma\"].append(mse)\n",
    "\n",
    "        _, mse, eff = quantize_group(weights, bits, group_size=128)\n",
    "        results[\"group_128\"].append(mse)\n",
    "        results[\"group_128_rate\"].append(eff)\n",
    "\n",
    "        _, mse, eff = quantize_group(weights, bits, group_size=32)\n",
    "        results[\"group_32\"].append(mse)\n",
    "        results[\"group_32_rate\"].append(eff)\n",
    "\n",
    "        _, mse = quantize_lloyd_max(weights, bits)\n",
    "        results[\"lloyd_max\"].append(mse)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_rate_distortion_scalar(results, title=\"Rate-Distortion (Scalar and Group)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bits = np.array(results[\"bits\"], dtype=np.float32)\n",
    "    plt.semilogy(bits, results[\"shannon_d\"], 'k--', linewidth=2, marker='o', label='Shannon bound')\n",
    "\n",
    "    methods = [\n",
    "        (\"uniform_asym\", \"Uniform (naive)\", 'red'),\n",
    "        (\"uniform_sym\", \"Uniform (symmetric)\", 'orange'),\n",
    "        (\"clipped_3sigma\", \"Clipped 3sigma\", 'green'),\n",
    "        (\"lloyd_max\", \"Lloyd-Max\", 'cyan'),\n",
    "    ]\n",
    "\n",
    "    for key, label, color in methods:\n",
    "        plt.semilogy(bits, results[key], '-', linewidth=1.5, marker='s', label=label, color=color)\n",
    "\n",
    "    plt.semilogy(results[\"group_128_rate\"], results[\"group_128\"], '-', linewidth=1.5, marker='^', label='Group g=128', color='blue')\n",
    "    plt.semilogy(results[\"group_32_rate\"], results[\"group_32\"], '-', linewidth=1.5, marker='^', label='Group g=32', color='purple')\n",
    "\n",
    "    plt.xlabel('Rate (bits per weight)')\n",
    "    plt.ylabel('Distortion (MSE)')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector and product quantization helpers\n",
    "\n",
    "def measure_vector_quantizers(weights, dims=(2, 4, 8), codebook_bits=6):\n",
    "    curves = []\n",
    "    for d in dims:\n",
    "        print(f\"  VQ: dim={d}, codebook_bits={codebook_bits}\")\n",
    "        _, mse, eff_bits = quantize_kmeans_vq(\n",
    "            weights,\n",
    "            bits=codebook_bits,\n",
    "            dim=d,\n",
    "            sample_vectors=10000,\n",
    "            eval_vectors=10000,\n",
    "            seed=0,\n",
    "        )\n",
    "        curves.append({\n",
    "            \"label\": f\"VQ d={d} (codebook {codebook_bits} bits)\",\n",
    "            \"rates\": [eff_bits],\n",
    "            \"mses\": [mse],\n",
    "        })\n",
    "    return curves\n",
    "\n",
    "\n",
    "def measure_product_quantizers(weights, dims=(2, 4, 8), codebook_bits=6, codebooks=4):\n",
    "    curves = []\n",
    "    for d in dims:\n",
    "        print(f\"  PQ: dim={d}, codebook_bits={codebook_bits}, codebooks={codebooks}\")\n",
    "        mse, eff_bits = quantize_product_quantization(\n",
    "            weights,\n",
    "            bits=codebook_bits,\n",
    "            dim=d,\n",
    "            codebooks=codebooks,\n",
    "            sample_vectors=10000,\n",
    "            eval_vectors=10000,\n",
    "            seed=0,\n",
    "        )\n",
    "        curves.append({\n",
    "            \"label\": f\"PQ d={d} (codebook {codebook_bits} bits, {codebooks} codebooks)\",\n",
    "            \"rates\": [eff_bits],\n",
    "            \"mses\": [mse],\n",
    "        })\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate-distortion analysis for a single layer\n",
    "layer = 8\n",
    "weights = mlp_weights[layer]\n",
    "\n",
    "print(f\"Analyzing MLP layer {layer}\")\n",
    "print(f\"Shape: {weights.shape}, Total weights: {weights.size:,}\")\n",
    "print(f\"Variance: {np.var(weights.astype(np.float32)):.6e}\")\n",
    "\n",
    "scalar_results = measure_scalar_quantizers(weights, bits_list=[2, 3, 4, 5, 6, 8])\n",
    "plot_rate_distortion_scalar(scalar_results, title=f\"Rate-Distortion: MLP Layer {layer}\")\n",
    "\n",
    "vq_curves = measure_vector_quantizers(weights, dims=(2, 4, 8), codebook_bits=6)\n",
    "pq_curves = measure_product_quantizers(weights, dims=(2, 4, 8), codebook_bits=6, codebooks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay vector and product quantization points on scalar curves\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bits = np.array(scalar_results[\"bits\"], dtype=np.float32)\n",
    "plt.semilogy(bits, scalar_results[\"shannon_d\"], 'k--', linewidth=2, marker='o', label='Shannon bound')\n",
    "\n",
    "for key, label, color in [\n",
    "    (\"uniform_sym\", \"Uniform (symmetric)\", 'orange'),\n",
    "    (\"clipped_3sigma\", \"Clipped 3sigma\", 'green'),\n",
    "    (\"lloyd_max\", \"Lloyd-Max\", 'cyan'),\n",
    "]:\n",
    "    plt.semilogy(bits, scalar_results[key], '-', linewidth=1.5, marker='s', label=label, color=color)\n",
    "\n",
    "plt.semilogy(scalar_results[\"group_128_rate\"], scalar_results[\"group_128\"], '-', linewidth=1.5, marker='^', label='Group g=128', color='blue')\n",
    "plt.semilogy(scalar_results[\"group_32_rate\"], scalar_results[\"group_32\"], '-', linewidth=1.5, marker='^', label='Group g=32', color='purple')\n",
    "\n",
    "for curve in vq_curves + pq_curves:\n",
    "    plt.semilogy(curve[\"rates\"], curve[\"mses\"], 'D', markersize=7, label=curve[\"label\"])\n",
    "\n",
    "plt.xlabel('Rate (bits per weight)')\n",
    "plt.ylabel('Distortion (MSE)')\n",
    "plt.title(f\"Rate-Distortion (MLP layer {layer})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap at target distortions\n",
    "\n",
    "sigma_sq = float(np.var(weights.flatten().astype(np.float32)))\n",
    "\n",
    "targets = [0.001 * sigma_sq, 0.01 * sigma_sq]\n",
    "\n",
    "print(\"Target distortions:\")\n",
    "for t in targets:\n",
    "    print(f\"  D = {t:.3e}\")\n",
    "\n",
    "curves = [\n",
    "    (\"Uniform (symmetric)\", scalar_results[\"bits\"], scalar_results[\"uniform_sym\"]),\n",
    "    (\"Clipped 3sigma\", scalar_results[\"bits\"], scalar_results[\"clipped_3sigma\"]),\n",
    "    (\"Group g=128\", scalar_results[\"group_128_rate\"], scalar_results[\"group_128\"]),\n",
    "    (\"Group g=32\", scalar_results[\"group_32_rate\"], scalar_results[\"group_32\"]),\n",
    "    (\"Lloyd-Max\", scalar_results[\"bits\"], scalar_results[\"lloyd_max\"]),\n",
    "]\n",
    "\n",
    "for t in targets:\n",
    "    print(f\"\\n=== Gap summary at D={t:.3e} ===\")\n",
    "    for label, rates, mses in curves:\n",
    "        r_emp = interpolate_rate_for_distortion(rates, mses, t)\n",
    "        if r_emp is None:\n",
    "            print(f\"  {label:<16} : insufficient coverage\")\n",
    "            continue\n",
    "        r_shannon = 0.5 * math.log2(sigma_sq / t)\n",
    "        gap = r_emp - r_shannon\n",
    "        print(f\"  {label:<16} : R_emp={r_emp:.2f} bits, gap={gap:.2f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-group comparison (MLP vs Attention vs Embeddings)\n",
    "\n",
    "bits_list = list(range(1, 9))\n",
    "\n",
    "\n",
    "def select_layers(layer_dict, max_layers=6):\n",
    "    layers = sorted(layer_dict.keys())\n",
    "    if len(layers) <= max_layers:\n",
    "        return layers\n",
    "    idx = np.linspace(0, len(layers) - 1, max_layers).round().astype(int)\n",
    "    return [layers[i] for i in idx]\n",
    "\n",
    "\n",
    "def avg_rd_curve(weights_list, bits_list, quantizer_fn):\n",
    "    mses = []\n",
    "    for bits in bits_list:\n",
    "        layer_mses = []\n",
    "        for W in weights_list:\n",
    "            _, mse = quantizer_fn(W, bits)\n",
    "            layer_mses.append(mse)\n",
    "        mses.append(float(np.mean(layer_mses)))\n",
    "    return mses\n",
    "\n",
    "mlp_layers = select_layers(mlp_weights, max_layers=6)\n",
    "q_layers = select_layers(attn_weights[\"q\"], max_layers=6)\n",
    "\n",
    "mlp_list = [mlp_weights[i] for i in mlp_layers]\n",
    "q_list = [attn_weights[\"q\"][i] for i in q_layers]\n",
    "\n",
    "mlp_curve = avg_rd_curve(mlp_list, bits_list, quantize_uniform_symmetric)\n",
    "q_curve = avg_rd_curve(q_list, bits_list, quantize_uniform_symmetric)\n",
    "\n",
    "emb_curve = None\n",
    "if \"token\" in embedding_weights:\n",
    "    emb_curve = [quantize_uniform_symmetric(embedding_weights[\"token\"], b)[1] for b in bits_list]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(bits_list, mlp_curve, '-o', label=f\"MLP down (avg {len(mlp_layers)} layers)\")\n",
    "plt.semilogy(bits_list, q_curve, '-o', label=f\"Attention Q (avg {len(q_layers)} layers)\")\n",
    "if emb_curve is not None:\n",
    "    plt.semilogy(bits_list, emb_curve, '-o', label=\"Token embeddings\")\n",
    "\n",
    "plt.xlabel('Rate (bits per weight)')\n",
    "plt.ylabel('Distortion (MSE)')\n",
    "plt.title('Per-group comparison (uniform symmetric)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
