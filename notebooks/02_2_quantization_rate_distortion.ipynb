{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Quantization: Bitsandbytes 4-bit Baselines\n",
    "\n",
    "**Objective:** Compare simple quantization methods and bitsandbytes 4-bit (NF4/FP4) against an FP16 baseline.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How do simple uniform methods compare to bitsandbytes NF4/FP4 at 4-bit?\n",
    "2. Does NF4 (Gaussian-optimized) beat FP4 (uniform log-spacing)? This validates our Gaussian assumption.\n",
    "3. Is weight MSE a good proxy for functional distortion (output MSE)?\n",
    "\n",
    "**Expected findings:**\n",
    "- NF4 should beat FP4, confirming near-Gaussian weight distributions\n",
    "- Simple uniform methods should underperform NF4/FP4 at equal bit-width\n",
    "- Weight importance should be non-uniform (functional MSE != weight MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUTHENTICATION (required) ===\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token directly as a string argument\n",
    "login(token=\"...\")\n",
    "\n",
    "# After running successfully, DELETE this cell or clear the token string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\n",
      "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\n",
      "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.1/632.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785e595489a84a03a73e794e35550c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a899c69f0d304ccaa1dd62d16c94f9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b6343d2c254383ab94b4544279c91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: meta-llama/Llama-3.2-1B\n",
      "Parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "# Load model + tokenizer (Llama 3.2 1B by default)\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# FP16 baseline\n",
    "m_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ").eval()\n",
    "\n",
    "# Keep the original variable name for downstream cells\n",
    "model = m_fp16\n",
    "\n",
    "print(f\"Loaded: {model_id}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions needed for baselines\n",
    "\n",
    "def symmetric_quantize(x, bits, per_channel=False, axis=1):\n",
    "    \"\"\"\n",
    "    Symmetric uniform quantization to signed integers.\n",
    "    per_channel=True uses per-output-channel scaling (axis=1 for [out, in]).\n",
    "    Returns dequantized array and scale(s).\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "    if per_channel:\n",
    "        max_abs = np.max(np.abs(x), axis=axis, keepdims=True) + 1e-12\n",
    "        scale = max_abs / qmax\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "    else:\n",
    "        max_abs = float(np.max(np.abs(x)))\n",
    "        scale = max_abs / qmax if max_abs > 0 else 1.0\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "\n",
    "    return dq.astype(np.float32), scale\n",
    "\n",
    "\n",
    "def shannon_distortion(sigma_sq, rate_bits):\n",
    "    return sigma_sq / (4 ** rate_bits)\n",
    "\n",
    "\n",
    "def gap_bits(mse, d_shannon):\n",
    "    if mse <= 0 or d_shannon <= 0:\n",
    "        return 0.0\n",
    "    return 0.5 * math.log2(mse / d_shannon)\n",
    "\n",
    "\n",
    "def quantize_group(weights, bits, group_size=128):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (group_size - n % group_size) % group_size\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    groups = flat_padded.reshape(-1, group_size)\n",
    "    levels = 2 ** bits\n",
    "    quantized_groups = np.zeros_like(groups)\n",
    "\n",
    "    for i in range(groups.shape[0]):\n",
    "        group = groups[i]\n",
    "        max_abs = float(np.max(np.abs(group)))\n",
    "        if max_abs == 0:\n",
    "            quantized_groups[i] = group\n",
    "            continue\n",
    "        scale = (2 * max_abs) / (levels - 1)\n",
    "        quantized_groups[i] = np.round(group / scale) * scale\n",
    "\n",
    "    quantized = quantized_groups.flatten()[:n]\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    effective_bits = bits + 16 / group_size\n",
    "    return quantized, mse, effective_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load bitsandbytes 4-bit Models (NF4/FP4)\n",
    "\n",
    "Use transformers + bitsandbytes to load 4-bit NF4 and FP4 checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load bitsandbytes 4-bit Quantized Models\n",
    "# ============================================\n",
    "\n",
    "# FP16 baseline already loaded as `model` (m_fp16)\n",
    "\n",
    "print(\"Loading bnb 4-bit NF4 model...\")\n",
    "try:\n",
    "    q_nf4 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "    m_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=q_nf4,\n",
    "    ).eval()\n",
    "\n",
    "    model_nf4 = m_nf4\n",
    "    NF4_AVAILABLE = True\n",
    "    print(\"✓ NF4 model loaded successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ NF4 model loading failed: {e}\")\n",
    "    print(\"  Install with: pip install bitsandbytes\")\n",
    "    model_nf4 = None\n",
    "    NF4_AVAILABLE = False\n",
    "\n",
    "print(\"\\nLoading bnb 4-bit FP4 model...\")\n",
    "try:\n",
    "    q_fp4 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"fp4\")\n",
    "    m_fp4 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=q_fp4,\n",
    "    ).eval()\n",
    "\n",
    "    model_fp4 = m_fp4\n",
    "    FP4_AVAILABLE = True\n",
    "    print(\"✓ FP4 model loaded successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ FP4 model loading failed: {e}\")\n",
    "    model_fp4 = None\n",
    "    FP4_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nModel availability summary:\")\n",
    "print(f\"  FP16 baseline: ✓ (already loaded)\")\n",
    "print(f\"  bnb 4-bit NF4: {'✓' if NF4_AVAILABLE else '✗'}\")\n",
    "print(f\"  bnb 4-bit FP4: {'✓' if FP4_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare Weights Across Models\n",
    "# ============================================\n",
    "\n",
    "# Target layer for comparison\n",
    "target_layer = 8\n",
    "target_proj = \"mlp.down_proj\"\n",
    "\n",
    "# Extract FP16 baseline weights (convert to fp32 for comparison)\n",
    "w_fp16 = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "def extract_module_weight(module):\n",
    "    \"\"\"Return dequantized weights for bnb 4-bit modules, or raw weights otherwise.\"\"\"\n",
    "    w = module.weight\n",
    "    if hasattr(w, \"quant_state\"):\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            w_deq = bnb.functional.dequantize_4bit(w.data, w.quant_state)\n",
    "            return w_deq.detach().cpu().float().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if hasattr(w, \"dequantize\"):\n",
    "            return w.dequantize().detach().cpu().float().numpy()\n",
    "    return w.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Analyzing layer {target_layer}, projection: {target_proj}\")\n",
    "print(f\"Weight shape: {w_fp16.shape}\")\n",
    "print(f\"FP16 variance: {np.var(w_fp16):.6e}\")\n",
    "print(f\"FP16 kurtosis: {np.mean(((w_fp16 - w_fp16.mean()) / w_fp16.std())**4):.2f}\")\n",
    "print()\n",
    "\n",
    "# Container for results\n",
    "sota_results = {\n",
    "    \"method\": [\"FP16 (baseline)\"],\n",
    "    \"mse\": [0.0],\n",
    "    \"bits_per_weight\": [16.0],\n",
    "    \"sqnr_db\": [float('inf')],\n",
    "}\n",
    "\n",
    "# Extract NF4 weights if available\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    try:\n",
    "        w_nf4 = extract_module_weight(model_nf4.model.layers[target_layer].mlp.down_proj)\n",
    "        mse_nf4 = np.mean((w_fp16 - w_nf4) ** 2)\n",
    "        sqnr_nf4 = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_nf4) if mse_nf4 > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"bnb 4-bit NF4\")\n",
    "        sota_results[\"mse\"].append(mse_nf4)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_nf4)\n",
    "        \n",
    "        print(f\"bnb 4-bit NF4:\")\n",
    "        print(f\"  MSE:     {mse_nf4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_nf4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_nf4)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract NF4 weights: {e}\\n\")\n",
    "\n",
    "# Extract FP4 weights if available\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    try:\n",
    "        w_fp4 = extract_module_weight(model_fp4.model.layers[target_layer].mlp.down_proj)\n",
    "        mse_fp4 = np.mean((w_fp16 - w_fp4) ** 2)\n",
    "        sqnr_fp4 = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_fp4) if mse_fp4 > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"bnb 4-bit FP4\")\n",
    "        sota_results[\"mse\"].append(mse_fp4)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_fp4)\n",
    "        \n",
    "        print(f\"bnb 4-bit FP4:\")\n",
    "        print(f\"  MSE:     {mse_fp4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_fp4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_fp4)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract FP4 weights: {e}\\n\")\n",
    "\n",
    "# Add simple methods for comparison (4-bit)\n",
    "print(\"Comparing to simple quantization methods:\")\n",
    "\n",
    "# Per-tensor symmetric 4-bit\n",
    "dq_per_tensor, _ = symmetric_quantize(w_fp16, 4, per_channel=False)\n",
    "mse_per_tensor = np.mean((w_fp16 - dq_per_tensor) ** 2)\n",
    "sqnr_per_tensor = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_tensor)\n",
    "sota_results[\"method\"].append(\"Simple per-tensor\")\n",
    "sota_results[\"mse\"].append(mse_per_tensor)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_tensor)\n",
    "print(f\"  Simple per-tensor 4-bit: MSE={mse_per_tensor:.2e}, SQNR={sqnr_per_tensor:.2f} dB\")\n",
    "\n",
    "# Per-channel symmetric 4-bit\n",
    "dq_per_channel, _ = symmetric_quantize(w_fp16, 4, per_channel=True, axis=1)\n",
    "mse_per_channel = np.mean((w_fp16 - dq_per_channel) ** 2)\n",
    "sqnr_per_channel = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_channel)\n",
    "sota_results[\"method\"].append(\"Simple per-channel\")\n",
    "sota_results[\"mse\"].append(mse_per_channel)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_channel)\n",
    "print(f\"  Simple per-channel 4-bit: MSE={mse_per_channel:.2e}, SQNR={sqnr_per_channel:.2f} dB\")\n",
    "\n",
    "# Group quantization (g=128)\n",
    "_, mse_group128, eff_bits_g128 = quantize_group(w_fp16, 4, group_size=128)\n",
    "sqnr_group128 = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_group128)\n",
    "sota_results[\"method\"].append(\"Group g=128\")\n",
    "sota_results[\"mse\"].append(mse_group128)\n",
    "sota_results[\"bits_per_weight\"].append(eff_bits_g128)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_group128)\n",
    "print(f\"  Group g=128 (4-bit): MSE={mse_group128:.2e}, SQNR={sqnr_group128:.2f} dB, eff_bits={eff_bits_g128:.3f}\")\n",
    "\n",
    "# Shannon bound (Gaussian assumption)\n",
    "sigma_sq = np.var(w_fp16)\n",
    "d_shannon_4bit = shannon_distortion(sigma_sq, 4.0)\n",
    "sqnr_shannon = 10 * np.log10(np.mean(w_fp16 ** 2) / d_shannon_4bit)\n",
    "sota_results[\"method\"].append(\"Shannon bound (4-bit)\")\n",
    "sota_results[\"mse\"].append(d_shannon_4bit)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "print(f\"  Shannon bound (4-bit): MSE={d_shannon_4bit:.2e}, SQNR={sqnr_shannon:.2f} dB\")\n",
    "print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_sota = pd.DataFrame(sota_results)\n",
    "df_sota = df_sota.sort_values(\"mse\", ascending=True)\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE (sorted by MSE, lower is better)\")\n",
    "print(\"=\" * 70)\n",
    "print(df_sota.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize 4-bit Comparison\n",
    "# ============================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MSE comparison (bar chart)\n",
    "methods_plot = [m for m in df_sota[\"method\"] if m != \"FP16 (baseline)\"]\n",
    "mses_plot = [df_sota[df_sota[\"method\"] == m][\"mse\"].values[0] for m in methods_plot]\n",
    "\n",
    "colors = []\n",
    "for m in methods_plot:\n",
    "    if \"Shannon\" in m:\n",
    "        colors.append('black')\n",
    "    elif \"NF4\" in m:\n",
    "        colors.append('green')\n",
    "    elif \"FP4\" in m:\n",
    "        colors.append('orange')\n",
    "    elif \"Group\" in m:\n",
    "        colors.append('blue')\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "ax1.bar(range(len(methods_plot)), mses_plot, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(methods_plot)))\n",
    "ax1.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel('MSE (log scale)')\n",
    "ax1.set_title(f'Weight MSE Comparison (Layer {target_layer})')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: SQNR comparison (higher is better)\n",
    "sqnrs_plot = [df_sota[df_sota[\"method\"] == m][\"sqnr_db\"].values[0] for m in methods_plot]\n",
    "sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs_plot]\n",
    "\n",
    "ax2.bar(range(len(methods_plot)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods_plot)))\n",
    "ax2.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax2.set_ylabel('SQNR (dB)')\n",
    "ax2.set_title(f'Signal-to-Quantization-Noise Ratio (Layer {target_layer})')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=sqnr_shannon, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Shannon bound')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display gaps\n",
    "print(\"\\nGap analysis (bits from Shannon bound at 4-bit):\")\n",
    "print(\"=\" * 60)\n",
    "for method in methods_plot:\n",
    "    if \"Shannon\" in method:\n",
    "        continue\n",
    "    row = df_sota[df_sota[\"method\"] == method].iloc[0]\n",
    "    mse = row[\"mse\"]\n",
    "    gap = gap_bits(mse, d_shannon_4bit)\n",
    "    print(f\"{method:25s}: {gap:+.3f} bits gap\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Compare bitsandbytes NF4 vs FP4\n",
    "\n",
    "**Hypothesis:** If weights are near-Gaussian, NF4 (optimized for Gaussian) should outperform FP4 (uniform log-spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Bitsandbytes models already loaded above\n",
    "# ============================================\n",
    "\n",
    "NF4_AVAILABLE = NF4_AVAILABLE if \"NF4_AVAILABLE\" in globals() else False\n",
    "FP4_AVAILABLE = FP4_AVAILABLE if \"FP4_AVAILABLE\" in globals() else False\n",
    "\n",
    "print(\"Using preloaded bitsandbytes models:\")\n",
    "print(f\"  NF4: {'✓' if NF4_AVAILABLE else '✗'}\")\n",
    "print(f\"  FP4: {'✓' if FP4_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare NF4 vs FP4 Weights\n",
    "# ============================================\n",
    "\n",
    "bnb_results = {\n",
    "    \"method\": [],\n",
    "    \"mse\": [],\n",
    "    \"sqnr_db\": [],\n",
    "}\n",
    "\n",
    "# We already have FP16 baseline from earlier\n",
    "w_fp16_ref = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Comparing NF4 vs FP4 on layer {target_layer}, {target_proj}\")\n",
    "print(f\"FP16 reference shape: {w_fp16_ref.shape}\")\n",
    "print()\n",
    "\n",
    "if \"extract_module_weight\" not in globals():\n",
    "    def extract_module_weight(module):\n",
    "        \"\"\"Return dequantized weights for bnb 4-bit modules, or raw weights otherwise.\"\"\"\n",
    "        w = module.weight\n",
    "        if hasattr(w, \"quant_state\"):\n",
    "            try:\n",
    "                import bitsandbytes as bnb\n",
    "                w_deq = bnb.functional.dequantize_4bit(w.data, w.quant_state)\n",
    "                return w_deq.detach().cpu().float().numpy()\n",
    "            except Exception:\n",
    "                pass\n",
    "            if hasattr(w, \"dequantize\"):\n",
    "                return w.dequantize().detach().cpu().float().numpy()\n",
    "        return w.detach().cpu().float().numpy()\n",
    "\n",
    "# Extract NF4 weights\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    try:\n",
    "        w_nf4 = extract_module_weight(model_nf4.model.layers[target_layer].mlp.down_proj)\n",
    "        \n",
    "        mse_nf4 = np.mean((w_fp16_ref - w_nf4) ** 2)\n",
    "        sqnr_nf4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_nf4) if mse_nf4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"bnb 4-bit NF4\")\n",
    "        bnb_results[\"mse\"].append(mse_nf4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_nf4)\n",
    "        \n",
    "        print(f\"bnb 4-bit NF4 (Gaussian-optimized):\")\n",
    "        print(f\"  MSE:     {mse_nf4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_nf4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_nf4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract NF4 weights: {e}\\n\")\n",
    "\n",
    "# Extract FP4 weights\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    try:\n",
    "        w_fp4 = extract_module_weight(model_fp4.model.layers[target_layer].mlp.down_proj)\n",
    "        \n",
    "        mse_fp4 = np.mean((w_fp16_ref - w_fp4) ** 2)\n",
    "        sqnr_fp4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_fp4) if mse_fp4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"bnb 4-bit FP4\")\n",
    "        bnb_results[\"mse\"].append(mse_fp4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_fp4)\n",
    "        \n",
    "        print(f\"bnb 4-bit FP4 (uniform log-spacing):\")\n",
    "        print(f\"  MSE:     {mse_fp4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_fp4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_fp4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract FP4 weights: {e}\\n\")\n",
    "\n",
    "# Add Shannon bound for reference\n",
    "bnb_results[\"method\"].append(\"Shannon bound\")\n",
    "bnb_results[\"mse\"].append(d_shannon_4bit)\n",
    "bnb_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "\n",
    "# Display comparison\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    df_bnb = pd.DataFrame(bnb_results)\n",
    "    df_bnb = df_bnb.sort_values(\"mse\", ascending=True)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NF4 vs FP4 COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_bnb.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Interpretation\n",
    "    if NF4_AVAILABLE and FP4_AVAILABLE:\n",
    "        improvement = (mse_fp4 - mse_nf4) / mse_fp4 * 100 if mse_fp4 > 0 else 0\n",
    "        print(f\"\\nNF4 achieves {improvement:.1f}% lower MSE than FP4\")\n",
    "        if improvement > 5:\n",
    "            print(\"✓ VALIDATION: NF4 significantly outperforms FP4\")\n",
    "            print(\"  → Confirms weights are approximately Gaussian\")\n",
    "        else:\n",
    "            print(\"⚠ UNEXPECTED: NF4 and FP4 perform similarly\")\n",
    "            print(\"  → Weights may be more heavy-tailed than expected\")\n",
    "else:\n",
    "    print(\"⚠ Could not load bitsandbytes models for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize NF4 vs FP4 Comparison\n",
    "# ============================================\n",
    "\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    methods = df_bnb[\"method\"].tolist()\n",
    "    mses = df_bnb[\"mse\"].tolist()\n",
    "    sqnrs = df_bnb[\"sqnr_db\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'NF4' in m else 'orange' if 'FP4' in m else 'black' for m in methods]\n",
    "    \n",
    "    # MSE comparison\n",
    "    ax1.bar(range(len(methods)), mses, color=colors, alpha=0.7)\n",
    "    ax1.set_xticks(range(len(methods)))\n",
    "    ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel('MSE (log scale)')\n",
    "    ax1.set_title('NF4 vs FP4: Weight MSE')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # SQNR comparison\n",
    "    sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs]\n",
    "    ax2.bar(range(len(methods)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('SQNR (dB)')\n",
    "    ax2.set_title('NF4 vs FP4: Signal Quality')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gap analysis\n",
    "    print(\"\\nGap from Shannon bound:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, mse in zip(methods, mses):\n",
    "        if \"Shannon\" not in method:\n",
    "            gap = gap_bits(mse, d_shannon_4bit)\n",
    "            print(f\"{method:25s}: {gap:+.3f} bits\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"Visualization skipped - bitsandbytes models not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Measure Functional Distortion\n",
    "\n",
    "**Key Question:** Is weight MSE a good proxy for output distortion?\n",
    "\n",
    "Insight: Some weights matter more than others. If weight importance is non-uniform, weight MSE won't perfectly correlate with functional MSE (output logits error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Prepare Calibration Data\n",
    "# ============================================\n",
    "\n",
    "print(\"Preparing calibration dataset for functional distortion measurement...\")\n",
    "\n",
    "calibration_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The weather today is sunny with a chance of rain.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Transformers have revolutionized natural language processing.\",\n",
    "    \"Climate change is one of the biggest challenges facing humanity.\",\n",
    "    \"The stock market experienced volatility in recent months.\",\n",
    "    \"Quantum computing promises to solve complex problems faster.\",\n",
    "    \"Renewable energy sources include solar, wind, and hydroelectric power.\",\n",
    "    \"The human brain contains approximately 86 billion neurons.\",\n",
    "]\n",
    "\n",
    "tokenizer = tok\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "calibration_inputs = tokenizer(\n",
    "    calibration_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"Calibration dataset: {len(calibration_texts)} samples\")\n",
    "print(f\"Input shape: {calibration_inputs['input_ids'].shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Compute Functional Distortion (Logits MSE)\n",
    "# ============================================\n",
    "\n",
    "def compute_functional_mse(model1, model2, inputs, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute MSE between output logits of two models.\n",
    "    \"\"\"\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs2 = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits1 = outputs1.logits.cpu().float()\n",
    "        logits2 = outputs2.logits.cpu().float()\n",
    "        \n",
    "        functional_mse = torch.mean((logits1 - logits2) ** 2).item()\n",
    "        per_token_mse = torch.mean((logits1 - logits2) ** 2, dim=(0, 2)).numpy()\n",
    "        \n",
    "    return functional_mse, per_token_mse\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "functional_results = {\n",
    "    \"method\": [],\n",
    "    \"weight_mse\": [],\n",
    "    \"functional_mse\": [],\n",
    "    \"ratio\": [],\n",
    "}\n",
    "\n",
    "print(\"Computing functional distortion...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "models_to_compare = []\n",
    "\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    models_to_compare.append((\"bnb 4-bit NF4\", model_nf4, mse_nf4 if 'mse_nf4' in locals() else None))\n",
    "\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    models_to_compare.append((\"bnb 4-bit FP4\", model_fp4, mse_fp4 if 'mse_fp4' in locals() else None))\n",
    "\n",
    "for method_name, quant_model, weight_mse in models_to_compare:\n",
    "    try:\n",
    "        print(f\"Computing functional MSE for {method_name}...\")\n",
    "        func_mse, per_token = compute_functional_mse(model, quant_model, calibration_inputs, device=device)\n",
    "        \n",
    "        functional_results[\"method\"].append(method_name)\n",
    "        functional_results[\"weight_mse\"].append(weight_mse if weight_mse is not None else np.nan)\n",
    "        functional_results[\"functional_mse\"].append(func_mse)\n",
    "        functional_results[\"ratio\"].append(func_mse / weight_mse if weight_mse and weight_mse > 0 else np.nan)\n",
    "        \n",
    "        print(f\"  Weight MSE:     {weight_mse:.2e}\" if weight_mse else \"  Weight MSE:     N/A\")\n",
    "        print(f\"  Functional MSE: {func_mse:.2e}\")\n",
    "        print(f\"  Ratio (F/W):    {func_mse / weight_mse:.2e}\" if weight_mse and weight_mse > 0 else \"  Ratio (F/W):    N/A\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    df_functional = pd.DataFrame(functional_results)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FUNCTIONAL DISTORTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_functional.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"⚠ No models available for functional distortion comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Weight MSE vs Functional MSE\n",
    "# ============================================\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    valid_mask = ~np.isnan(df_functional[\"weight_mse\"]) & ~np.isnan(df_functional[\"functional_mse\"])\n",
    "    df_valid = df_functional[valid_mask]\n",
    "    \n",
    "    if len(df_valid) > 0:\n",
    "        ax1.scatter(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"], s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in df_valid.iterrows():\n",
    "            ax1.annotate(row[\"method\"], \n",
    "                        (row[\"weight_mse\"], row[\"functional_mse\"]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        ax1.set_xlabel('Weight MSE (log scale)')\n",
    "        ax1.set_ylabel('Functional MSE (log scale)')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title('Weight MSE vs Functional MSE')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        xlim = ax1.get_xlim()\n",
    "        ylim = ax1.get_ylim()\n",
    "        min_val = max(xlim[0], ylim[0])\n",
    "        max_val = min(xlim[1], ylim[1])\n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3, linewidth=1, label='y=x')\n",
    "        ax1.legend()\n",
    "    \n",
    "    methods = df_functional[\"method\"].tolist()\n",
    "    func_mses = df_functional[\"functional_mse\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'NF4' in m else 'orange' if 'FP4' in m else 'blue' for m in methods]\n",
    "    \n",
    "    ax2.bar(range(len(methods)), func_mses, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Functional MSE (log scale)')\n",
    "    ax2.set_title('Output Logits Error by Method')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if len(df_valid) > 1:\n",
    "        from scipy.stats import spearmanr, pearsonr\n",
    "        \n",
    "        corr_pearson, p_pearson = pearsonr(np.log(df_valid[\"weight_mse\"]), np.log(df_valid[\"functional_mse\"]))\n",
    "        corr_spearman, p_spearman = spearmanr(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"])\n",
    "        \n",
    "        print(\"\\nCorrelation Analysis:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Pearson correlation (log-log):  r={corr_pearson:.3f}, p={p_pearson:.4f}\")\n",
    "        print(f\"Spearman correlation (rank):    ρ={corr_spearman:.3f}, p={p_spearman:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if corr_spearman > 0.8:\n",
    "            print(\"✓ Strong correlation: Weight MSE is a good proxy for functional MSE\")\n",
    "        elif corr_spearman > 0.5:\n",
    "            print(\"⚠ Moderate correlation: Weight MSE partially predicts functional MSE\")\n",
    "        else:\n",
    "            print(\"✗ Weak correlation: Weight importance is highly non-uniform\")\n",
    "            print(\"  → Consider per-channel or groupwise importance weighting\")\n",
    "else:\n",
    "    print(\"Visualization skipped - no functional distortion data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary: Key Findings from 4-bit Comparison\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "1. **bitsandbytes vs Simple Methods:**\n",
    "   - bnb 4-bit NF4/FP4 should outperform simple uniform quantization at the same bit-width\n",
    "   - The gap reflects better quantization codebooks for common weight distributions\n",
    "\n",
    "2. **NF4 vs FP4:**\n",
    "   - If NF4 >> FP4: Confirms weights are approximately Gaussian\n",
    "   - If NF4 ≈ FP4: Weights may be heavier-tailed than expected\n",
    "\n",
    "3. **Weight MSE vs Functional MSE:**\n",
    "   - Strong correlation (ρ > 0.8): Weight MSE is a good proxy, uniform importance\n",
    "   - Weak correlation (ρ < 0.5): Weight importance is non-uniform, per-channel schemes critical\n",
    "\n",
    "**Research Opportunities:**\n",
    "- If simple methods are close to NF4/FP4: tighten groupwise scaling or activation-aware schemes\n",
    "- If NF4 wins significantly: explore optimal quantization levels for near-Gaussian distributions\n",
    "- If functional MSE deviates: investigate per-layer and per-channel importance patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
