{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Quantization: State-of-the-Art Methods\n",
    "\n",
    "**Objective:** Compare simple quantization methods against production-grade GPTQ, AWQ, and bitsandbytes to understand the gap.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How much better is GPTQ/AWQ (with Hessian-based error compensation) than simple uniform quantization?\n",
    "2. Does NF4 (Gaussian-optimized) beat FP4 (uniform log-spacing)? This validates our Gaussian assumption.\n",
    "3. Is weight MSE a good proxy for functional distortion (output MSE)?\n",
    "\n",
    "**Expected findings:**\n",
    "- GPTQ/AWQ should achieve ~0.5-1.0 bits better than simple methods due to error compensation\n",
    "- NF4 should beat FP4, confirming near-Gaussian weight distributions\n",
    "- Weight importance should be non-uniform (functional MSE != weight MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUTHENTICATION (required) ===\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token directly as a string argument\n",
    "login(token=\"...\")\n",
    "\n",
    "# After running successfully, DELETE this cell or clear the token string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:c29774f5-fb4b-4339-9745-5d3420342175"
     ]
    }
   ],
   "source": [
    "# === Install dependencies ===\n",
    "# Uncomment the lines below to install required packages\n",
    "\n",
    "%pip install -q torch torchvision\n",
    "%pip install -q numpy scipy scikit-learn matplotlib pandas transformers accelerate\n",
    "%pip install -q bitsandbytes  # For NF4/FP4 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (Llama 3.2 1B by default)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions needed for baselines\n",
    "\n",
    "def symmetric_quantize(x, bits, per_channel=False, axis=1):\n",
    "    \"\"\"\n",
    "    Symmetric uniform quantization to signed integers.\n",
    "    per_channel=True uses per-output-channel scaling (axis=1 for [out, in]).\n",
    "    Returns dequantized array and scale(s).\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "    if per_channel:\n",
    "        max_abs = np.max(np.abs(x), axis=axis, keepdims=True) + 1e-12\n",
    "        scale = max_abs / qmax\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "    else:\n",
    "        max_abs = float(np.max(np.abs(x)))\n",
    "        scale = max_abs / qmax if max_abs > 0 else 1.0\n",
    "        q = np.clip(np.round(x / scale), -qmax, qmax)\n",
    "        dq = q * scale\n",
    "\n",
    "    return dq.astype(np.float32), scale\n",
    "\n",
    "\n",
    "def shannon_distortion(sigma_sq, rate_bits):\n",
    "    return sigma_sq / (4 ** rate_bits)\n",
    "\n",
    "\n",
    "def gap_bits(mse, d_shannon):\n",
    "    if mse <= 0 or d_shannon <= 0:\n",
    "        return 0.0\n",
    "    return 0.5 * math.log2(mse / d_shannon)\n",
    "\n",
    "\n",
    "def quantize_group(weights, bits, group_size=128):\n",
    "    flat = weights.flatten().astype(np.float32)\n",
    "    n = len(flat)\n",
    "\n",
    "    pad_size = (group_size - n % group_size) % group_size\n",
    "    if pad_size > 0:\n",
    "        flat_padded = np.concatenate([flat, np.zeros(pad_size, dtype=np.float32)])\n",
    "    else:\n",
    "        flat_padded = flat\n",
    "\n",
    "    groups = flat_padded.reshape(-1, group_size)\n",
    "    levels = 2 ** bits\n",
    "    quantized_groups = np.zeros_like(groups)\n",
    "\n",
    "    for i in range(groups.shape[0]):\n",
    "        group = groups[i]\n",
    "        max_abs = float(np.max(np.abs(group)))\n",
    "        if max_abs == 0:\n",
    "            quantized_groups[i] = group\n",
    "            continue\n",
    "        scale = (2 * max_abs) / (levels - 1)\n",
    "        quantized_groups[i] = np.round(group / scale) * scale\n",
    "\n",
    "    quantized = quantized_groups.flatten()[:n]\n",
    "    mse = float(np.mean((flat - quantized) ** 2))\n",
    "    effective_bits = bits + 16 / group_size\n",
    "    return quantized, mse, effective_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Pre-Quantized GPTQ/AWQ Models\n",
    "\n",
    "Install required libraries and load quantized checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load Original FP16 Model (already loaded above)\n",
    "# ============================================\n",
    "# We'll reuse the 'model' variable from earlier as our FP16 baseline\n",
    "\n",
    "# ============================================\n",
    "# Load AWQ 4-bit Quantized Model\n",
    "# ============================================\n",
    "print(\"Loading AWQ 4-bit model...\")\n",
    "try:\n",
    "    model_awq = AutoModelForCausalLM.from_pretrained(\n",
    "        \"AMead10/Llama-3.2-1B-Instruct-AWQ\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ AWQ model loaded successfully\")\n",
    "    AWQ_AVAILABLE = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ AWQ model loading failed: {e}\")\n",
    "    print(\"  Try upgrading transformers or verifying the checkpoint supports HF quantized weights\")\n",
    "    model_awq = None\n",
    "    AWQ_AVAILABLE = False\n",
    "\n",
    "# ============================================\n",
    "# Load GPTQ 4-bit Quantized Model\n",
    "# ============================================\n",
    "print(\"Loading GPTQ 4-bit model...\")\n",
    "try:\n",
    "    model_gptq = AutoModelForCausalLM.from_pretrained(\n",
    "        \"clowman/Llama-3.2-1B-Instruct-GPTQ-Int4\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ GPTQ model loaded successfully\")\n",
    "    GPTQ_AVAILABLE = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ GPTQ model loading failed: {e}\")\n",
    "    print(\"  Try upgrading transformers or verifying the checkpoint supports HF quantized weights\")\n",
    "    model_gptq = None\n",
    "    GPTQ_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nModel availability summary:\")\n",
    "print(f\"  FP16 baseline: ✓ (already loaded)\")\n",
    "print(f\"  AWQ 4-bit: {'✓' if AWQ_AVAILABLE else '✗'}\")\n",
    "print(f\"  GPTQ 4-bit: {'✓' if GPTQ_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare Weights Across Models\n",
    "# ============================================\n",
    "\n",
    "# Target layer for comparison\n",
    "target_layer = 8\n",
    "target_proj = \"mlp.down_proj\"\n",
    "\n",
    "# Extract FP16 baseline weights (convert to fp32 for comparison)\n",
    "w_fp16 = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Analyzing layer {target_layer}, projection: {target_proj}\")\n",
    "print(f\"Weight shape: {w_fp16.shape}\")\n",
    "print(f\"FP16 variance: {np.var(w_fp16):.6e}\")\n",
    "print(f\"FP16 kurtosis: {np.mean(((w_fp16 - w_fp16.mean()) / w_fp16.std())**4):.2f}\")\n",
    "print()\n",
    "\n",
    "# Container for results\n",
    "sota_results = {\n",
    "    \"method\": [\"FP16 (baseline)\"],\n",
    "    \"mse\": [0.0],\n",
    "    \"bits_per_weight\": [16.0],\n",
    "    \"sqnr_db\": [float('inf')],\n",
    "}\n",
    "\n",
    "# Extract AWQ weights if available\n",
    "if AWQ_AVAILABLE and model_awq is not None:\n",
    "    try:\n",
    "        w_awq = model_awq.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "        mse_awq = np.mean((w_fp16 - w_awq) ** 2)\n",
    "        sqnr_awq = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_awq) if mse_awq > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"AWQ 4-bit\")\n",
    "        sota_results[\"mse\"].append(mse_awq)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_awq)\n",
    "        \n",
    "        print(f\"AWQ 4-bit:\")\n",
    "        print(f\"  MSE:     {mse_awq:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_awq:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_awq)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract AWQ weights: {e}\\n\")\n",
    "\n",
    "# Extract GPTQ weights if available\n",
    "if GPTQ_AVAILABLE and model_gptq is not None:\n",
    "    try:\n",
    "        w_gptq = model_gptq.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "        mse_gptq = np.mean((w_fp16 - w_gptq) ** 2)\n",
    "        sqnr_gptq = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_gptq) if mse_gptq > 0 else float('inf')\n",
    "        \n",
    "        sota_results[\"method\"].append(\"GPTQ 4-bit\")\n",
    "        sota_results[\"mse\"].append(mse_gptq)\n",
    "        sota_results[\"bits_per_weight\"].append(4.0)\n",
    "        sota_results[\"sqnr_db\"].append(sqnr_gptq)\n",
    "        \n",
    "        print(f\"GPTQ 4-bit:\")\n",
    "        print(f\"  MSE:     {mse_gptq:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_gptq:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16 - w_gptq)):.2e}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract GPTQ weights: {e}\\n\")\n",
    "\n",
    "# Add simple methods for comparison (4-bit)\n",
    "print(\"Comparing to simple quantization methods:\")\n",
    "\n",
    "# Per-tensor symmetric 4-bit\n",
    "dq_per_tensor, _ = symmetric_quantize(w_fp16, 4, per_channel=False)\n",
    "mse_per_tensor = np.mean((w_fp16 - dq_per_tensor) ** 2)\n",
    "sqnr_per_tensor = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_tensor)\n",
    "sota_results[\"method\"].append(\"Simple per-tensor\")\n",
    "sota_results[\"mse\"].append(mse_per_tensor)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_tensor)\n",
    "print(f\"  Simple per-tensor 4-bit: MSE={mse_per_tensor:.2e}, SQNR={sqnr_per_tensor:.2f} dB\")\n",
    "\n",
    "# Per-channel symmetric 4-bit\n",
    "dq_per_channel, _ = symmetric_quantize(w_fp16, 4, per_channel=True, axis=1)\n",
    "mse_per_channel = np.mean((w_fp16 - dq_per_channel) ** 2)\n",
    "sqnr_per_channel = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_per_channel)\n",
    "sota_results[\"method\"].append(\"Simple per-channel\")\n",
    "sota_results[\"mse\"].append(mse_per_channel)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_per_channel)\n",
    "print(f\"  Simple per-channel 4-bit: MSE={mse_per_channel:.2e}, SQNR={sqnr_per_channel:.2f} dB\")\n",
    "\n",
    "# Group quantization (g=128)\n",
    "_, mse_group128, eff_bits_g128 = quantize_group(w_fp16, 4, group_size=128)\n",
    "sqnr_group128 = 10 * np.log10(np.mean(w_fp16 ** 2) / mse_group128)\n",
    "sota_results[\"method\"].append(\"Group g=128\")\n",
    "sota_results[\"mse\"].append(mse_group128)\n",
    "sota_results[\"bits_per_weight\"].append(eff_bits_g128)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_group128)\n",
    "print(f\"  Group g=128 (4-bit): MSE={mse_group128:.2e}, SQNR={sqnr_group128:.2f} dB, eff_bits={eff_bits_g128:.3f}\")\n",
    "\n",
    "# Shannon bound (Gaussian assumption)\n",
    "sigma_sq = np.var(w_fp16)\n",
    "d_shannon_4bit = shannon_distortion(sigma_sq, 4.0)\n",
    "sqnr_shannon = 10 * np.log10(np.mean(w_fp16 ** 2) / d_shannon_4bit)\n",
    "sota_results[\"method\"].append(\"Shannon bound (4-bit)\")\n",
    "sota_results[\"mse\"].append(d_shannon_4bit)\n",
    "sota_results[\"bits_per_weight\"].append(4.0)\n",
    "sota_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "print(f\"  Shannon bound (4-bit): MSE={d_shannon_4bit:.2e}, SQNR={sqnr_shannon:.2f} dB\")\n",
    "print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_sota = pd.DataFrame(sota_results)\n",
    "df_sota = df_sota.sort_values(\"mse\", ascending=True)\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE (sorted by MSE, lower is better)\")\n",
    "print(\"=\" * 70)\n",
    "print(df_sota.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize SOTA Comparison\n",
    "# ============================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MSE comparison (bar chart)\n",
    "methods_plot = [m for m in df_sota[\"method\"] if m != \"FP16 (baseline)\"]\n",
    "mses_plot = [df_sota[df_sota[\"method\"] == m][\"mse\"].values[0] for m in methods_plot]\n",
    "\n",
    "colors = []\n",
    "for m in methods_plot:\n",
    "    if \"Shannon\" in m:\n",
    "        colors.append('black')\n",
    "    elif \"AWQ\" in m or \"GPTQ\" in m:\n",
    "        colors.append('green')\n",
    "    elif \"Group\" in m:\n",
    "        colors.append('blue')\n",
    "    else:\n",
    "        colors.append('orange')\n",
    "\n",
    "ax1.bar(range(len(methods_plot)), mses_plot, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(methods_plot)))\n",
    "ax1.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel('MSE (log scale)')\n",
    "ax1.set_title(f'Weight MSE Comparison (Layer {target_layer})')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: SQNR comparison (higher is better)\n",
    "sqnrs_plot = [df_sota[df_sota[\"method\"] == m][\"sqnr_db\"].values[0] for m in methods_plot]\n",
    "sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs_plot]\n",
    "\n",
    "ax2.bar(range(len(methods_plot)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods_plot)))\n",
    "ax2.set_xticklabels(methods_plot, rotation=45, ha='right')\n",
    "ax2.set_ylabel('SQNR (dB)')\n",
    "ax2.set_title(f'Signal-to-Quantization-Noise Ratio (Layer {target_layer})')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=sqnr_shannon, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Shannon bound')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display gaps\n",
    "print(\"\\nGap analysis (bits from Shannon bound at 4-bit):\")\n",
    "print(\"=\" * 60)\n",
    "for method in methods_plot:\n",
    "    if \"Shannon\" in method:\n",
    "        continue\n",
    "    row = df_sota[df_sota[\"method\"] == method].iloc[0]\n",
    "    mse = row[\"mse\"]\n",
    "    gap = gap_bits(mse, d_shannon_4bit)\n",
    "    print(f\"{method:25s}: {gap:+.3f} bits gap\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Compare bitsandbytes NF4 vs FP4\n",
    "\n",
    "**Hypothesis:** If weights are near-Gaussian, NF4 (optimized for Gaussian) should outperform FP4 (uniform log-spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load with NF4 (NormalFloat4 - optimized for Gaussian)\n",
    "# ============================================\n",
    "print(\"Loading NF4 (Gaussian-optimized) quantized model...\")\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    config_nf4 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False\n",
    "    )\n",
    "    \n",
    "    model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        quantization_config=config_nf4,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"✓ NF4 model loaded successfully\")\n",
    "    NF4_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ NF4 loading failed: {e}\")\n",
    "    print(\"  Install with: pip install bitsandbytes\")\n",
    "    model_nf4 = None\n",
    "    NF4_AVAILABLE = False\n",
    "\n",
    "# ============================================\n",
    "# Load with FP4 (uniform log-spacing)\n",
    "# ============================================\n",
    "print(\"\\nLoading FP4 (uniform log-spacing) quantized model...\")\n",
    "try:\n",
    "    config_fp4 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"fp4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False\n",
    "    )\n",
    "    \n",
    "    model_fp4 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        quantization_config=config_fp4,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"✓ FP4 model loaded successfully\")\n",
    "    FP4_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ FP4 loading failed: {e}\")\n",
    "    model_fp4 = None\n",
    "    FP4_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nbitsandbytes availability:\")\n",
    "print(f\"  NF4 (Gaussian-optimized): {'✓' if NF4_AVAILABLE else '✗'}\")\n",
    "print(f\"  FP4 (uniform log):        {'✓' if FP4_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract and Compare NF4 vs FP4 Weights\n",
    "# ============================================\n",
    "\n",
    "bnb_results = {\n",
    "    \"method\": [],\n",
    "    \"mse\": [],\n",
    "    \"sqnr_db\": [],\n",
    "}\n",
    "\n",
    "# We already have FP16 baseline from earlier\n",
    "w_fp16_ref = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
    "\n",
    "print(f\"Comparing NF4 vs FP4 on layer {target_layer}, {target_proj}\")\n",
    "print(f\"FP16 reference shape: {w_fp16_ref.shape}\")\n",
    "print()\n",
    "\n",
    "# Extract NF4 weights\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    try:\n",
    "        w_nf4_tensor = model_nf4.model.layers[target_layer].mlp.down_proj.weight\n",
    "        w_nf4 = w_nf4_tensor.detach().cpu().float().numpy()\n",
    "        \n",
    "        mse_nf4 = np.mean((w_fp16_ref - w_nf4) ** 2)\n",
    "        sqnr_nf4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_nf4) if mse_nf4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"NF4 (Gaussian-optimized)\")\n",
    "        bnb_results[\"mse\"].append(mse_nf4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_nf4)\n",
    "        \n",
    "        print(f\"NF4 (Gaussian-optimized):\")\n",
    "        print(f\"  MSE:     {mse_nf4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_nf4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_nf4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract NF4 weights: {e}\\n\")\n",
    "\n",
    "# Extract FP4 weights\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    try:\n",
    "        w_fp4_tensor = model_fp4.model.layers[target_layer].mlp.down_proj.weight\n",
    "        w_fp4 = w_fp4_tensor.detach().cpu().float().numpy()\n",
    "        \n",
    "        mse_fp4 = np.mean((w_fp16_ref - w_fp4) ** 2)\n",
    "        sqnr_fp4 = 10 * np.log10(np.mean(w_fp16_ref ** 2) / mse_fp4) if mse_fp4 > 0 else float('inf')\n",
    "        \n",
    "        bnb_results[\"method\"].append(\"FP4 (uniform log)\")\n",
    "        bnb_results[\"mse\"].append(mse_fp4)\n",
    "        bnb_results[\"sqnr_db\"].append(sqnr_fp4)\n",
    "        \n",
    "        print(f\"FP4 (uniform log-spacing):\")\n",
    "        print(f\"  MSE:     {mse_fp4:.2e}\")\n",
    "        print(f\"  SQNR:    {sqnr_fp4:.2f} dB\")\n",
    "        print(f\"  Max err: {np.max(np.abs(w_fp16_ref - w_fp4)):.2e}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract FP4 weights: {e}\\n\")\n",
    "\n",
    "# Add Shannon bound for reference\n",
    "bnb_results[\"method\"].append(\"Shannon bound\")\n",
    "bnb_results[\"mse\"].append(d_shannon_4bit)\n",
    "bnb_results[\"sqnr_db\"].append(sqnr_shannon)\n",
    "\n",
    "# Display comparison\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    df_bnb = pd.DataFrame(bnb_results)\n",
    "    df_bnb = df_bnb.sort_values(\"mse\", ascending=True)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NF4 vs FP4 COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_bnb.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Interpretation\n",
    "    if NF4_AVAILABLE and FP4_AVAILABLE:\n",
    "        improvement = (mse_fp4 - mse_nf4) / mse_fp4 * 100 if mse_fp4 > 0 else 0\n",
    "        print(f\"\\nNF4 achieves {improvement:.1f}% lower MSE than FP4\")\n",
    "        if improvement > 5:\n",
    "            print(\"✓ VALIDATION: NF4 significantly outperforms FP4\")\n",
    "            print(\"  → Confirms weights are approximately Gaussian\")\n",
    "        else:\n",
    "            print(\"⚠ UNEXPECTED: NF4 and FP4 perform similarly\")\n",
    "            print(\"  → Weights may be more heavy-tailed than expected\")\n",
    "else:\n",
    "    print(\"⚠ Could not load bitsandbytes models for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize NF4 vs FP4 Comparison\n",
    "# ============================================\n",
    "\n",
    "if len(bnb_results[\"method\"]) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    methods = df_bnb[\"method\"].tolist()\n",
    "    mses = df_bnb[\"mse\"].tolist()\n",
    "    sqnrs = df_bnb[\"sqnr_db\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'NF4' in m else 'orange' if 'FP4' in m else 'black' for m in methods]\n",
    "    \n",
    "    # MSE comparison\n",
    "    ax1.bar(range(len(methods)), mses, color=colors, alpha=0.7)\n",
    "    ax1.set_xticks(range(len(methods)))\n",
    "    ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel('MSE (log scale)')\n",
    "    ax1.set_title('NF4 vs FP4: Weight MSE')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # SQNR comparison\n",
    "    sqnrs_plot = [s if s != float('inf') else 100 for s in sqnrs]\n",
    "    ax2.bar(range(len(methods)), sqnrs_plot, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('SQNR (dB)')\n",
    "    ax2.set_title('NF4 vs FP4: Signal Quality')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gap analysis\n",
    "    print(\"\\nGap from Shannon bound:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, mse in zip(methods, mses):\n",
    "        if \"Shannon\" not in method:\n",
    "            gap = gap_bits(mse, d_shannon_4bit)\n",
    "            print(f\"{method:25s}: {gap:+.3f} bits\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"Visualization skipped - bitsandbytes models not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Measure Functional Distortion\n",
    "\n",
    "**Key Question:** Is weight MSE a good proxy for output distortion?\n",
    "\n",
    "AWQ's insight: Some weights matter more than others. If weight importance is non-uniform, we should see that weight MSE doesn't correlate perfectly with functional MSE (output logits error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Prepare Calibration Data\n",
    "# ============================================\n",
    "\n",
    "print(\"Preparing calibration dataset for functional distortion measurement...\")\n",
    "\n",
    "calibration_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The weather today is sunny with a chance of rain.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Transformers have revolutionized natural language processing.\",\n",
    "    \"Climate change is one of the biggest challenges facing humanity.\",\n",
    "    \"The stock market experienced volatility in recent months.\",\n",
    "    \"Quantum computing promises to solve complex problems faster.\",\n",
    "    \"Renewable energy sources include solar, wind, and hydroelectric power.\",\n",
    "    \"The human brain contains approximately 86 billion neurons.\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "calibration_inputs = tokenizer(\n",
    "    calibration_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"Calibration dataset: {len(calibration_texts)} samples\")\n",
    "print(f\"Input shape: {calibration_inputs['input_ids'].shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Compute Functional Distortion (Logits MSE)\n",
    "# ============================================\n",
    "\n",
    "def compute_functional_mse(model1, model2, inputs, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute MSE between output logits of two models.\n",
    "    \"\"\"\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs2 = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits1 = outputs1.logits.cpu().float()\n",
    "        logits2 = outputs2.logits.cpu().float()\n",
    "        \n",
    "        functional_mse = torch.mean((logits1 - logits2) ** 2).item()\n",
    "        per_token_mse = torch.mean((logits1 - logits2) ** 2, dim=(0, 2)).numpy()\n",
    "        \n",
    "    return functional_mse, per_token_mse\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "functional_results = {\n",
    "    \"method\": [],\n",
    "    \"weight_mse\": [],\n",
    "    \"functional_mse\": [],\n",
    "    \"ratio\": [],\n",
    "}\n",
    "\n",
    "print(\"Computing functional distortion...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "models_to_compare = []\n",
    "\n",
    "if AWQ_AVAILABLE and model_awq is not None:\n",
    "    models_to_compare.append((\"AWQ 4-bit\", model_awq, mse_awq if 'mse_awq' in locals() else None))\n",
    "\n",
    "if GPTQ_AVAILABLE and model_gptq is not None:\n",
    "    models_to_compare.append((\"GPTQ 4-bit\", model_gptq, mse_gptq if 'mse_gptq' in locals() else None))\n",
    "\n",
    "if NF4_AVAILABLE and model_nf4 is not None:\n",
    "    models_to_compare.append((\"NF4 4-bit\", model_nf4, mse_nf4 if 'mse_nf4' in locals() else None))\n",
    "\n",
    "if FP4_AVAILABLE and model_fp4 is not None:\n",
    "    models_to_compare.append((\"FP4 4-bit\", model_fp4, mse_fp4 if 'mse_fp4' in locals() else None))\n",
    "\n",
    "for method_name, quant_model, weight_mse in models_to_compare:\n",
    "    try:\n",
    "        print(f\"Computing functional MSE for {method_name}...\")\n",
    "        func_mse, per_token = compute_functional_mse(model, quant_model, calibration_inputs, device=device)\n",
    "        \n",
    "        functional_results[\"method\"].append(method_name)\n",
    "        functional_results[\"weight_mse\"].append(weight_mse if weight_mse is not None else np.nan)\n",
    "        functional_results[\"functional_mse\"].append(func_mse)\n",
    "        functional_results[\"ratio\"].append(func_mse / weight_mse if weight_mse and weight_mse > 0 else np.nan)\n",
    "        \n",
    "        print(f\"  Weight MSE:     {weight_mse:.2e}\" if weight_mse else \"  Weight MSE:     N/A\")\n",
    "        print(f\"  Functional MSE: {func_mse:.2e}\")\n",
    "        print(f\"  Ratio (F/W):    {func_mse / weight_mse:.2e}\" if weight_mse and weight_mse > 0 else \"  Ratio (F/W):    N/A\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    df_functional = pd.DataFrame(functional_results)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FUNCTIONAL DISTORTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_functional.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"⚠ No models available for functional distortion comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Weight MSE vs Functional MSE\n",
    "# ============================================\n",
    "\n",
    "if len(functional_results[\"method\"]) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    valid_mask = ~np.isnan(df_functional[\"weight_mse\"]) & ~np.isnan(df_functional[\"functional_mse\"])\n",
    "    df_valid = df_functional[valid_mask]\n",
    "    \n",
    "    if len(df_valid) > 0:\n",
    "        ax1.scatter(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"], s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in df_valid.iterrows():\n",
    "            ax1.annotate(row[\"method\"], \n",
    "                        (row[\"weight_mse\"], row[\"functional_mse\"]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        ax1.set_xlabel('Weight MSE (log scale)')\n",
    "        ax1.set_ylabel('Functional MSE (log scale)')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title('Weight MSE vs Functional MSE')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        xlim = ax1.get_xlim()\n",
    "        ylim = ax1.get_ylim()\n",
    "        min_val = max(xlim[0], ylim[0])\n",
    "        max_val = min(xlim[1], ylim[1])\n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3, linewidth=1, label='y=x')\n",
    "        ax1.legend()\n",
    "    \n",
    "    methods = df_functional[\"method\"].tolist()\n",
    "    func_mses = df_functional[\"functional_mse\"].tolist()\n",
    "    \n",
    "    colors = ['green' if 'AWQ' in m or 'GPTQ' in m else 'blue' if 'NF4' in m else 'orange' for m in methods]\n",
    "    \n",
    "    ax2.bar(range(len(methods)), func_mses, color=colors, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Functional MSE (log scale)')\n",
    "    ax2.set_title('Output Logits Error by Method')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if len(df_valid) > 1:\n",
    "        from scipy.stats import spearmanr, pearsonr\n",
    "        \n",
    "        corr_pearson, p_pearson = pearsonr(np.log(df_valid[\"weight_mse\"]), np.log(df_valid[\"functional_mse\"]))\n",
    "        corr_spearman, p_spearman = spearmanr(df_valid[\"weight_mse\"], df_valid[\"functional_mse\"])\n",
    "        \n",
    "        print(\"\\nCorrelation Analysis:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Pearson correlation (log-log):  r={corr_pearson:.3f}, p={p_pearson:.4f}\")\n",
    "        print(f\"Spearman correlation (rank):    ρ={corr_spearman:.3f}, p={p_spearman:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if corr_spearman > 0.8:\n",
    "            print(\"✓ Strong correlation: Weight MSE is a good proxy for functional MSE\")\n",
    "        elif corr_spearman > 0.5:\n",
    "            print(\"⚠ Moderate correlation: Weight MSE partially predicts functional MSE\")\n",
    "        else:\n",
    "            print(\"✗ Weak correlation: Weight importance is highly non-uniform\")\n",
    "            print(\"  → AWQ's per-channel importance weighting is crucial\")\n",
    "else:\n",
    "    print(\"Visualization skipped - no functional distortion data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary: Key Findings from SOTA Comparison\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "1. **GPTQ/AWQ vs Simple Methods:**\n",
    "   - GPTQ/AWQ should achieve 0.5-1.0 bits better compression at same distortion\n",
    "   - This gap comes from Hessian-based error compensation\n",
    "\n",
    "2. **NF4 vs FP4:**\n",
    "   - If NF4 >> FP4: Confirms weights are approximately Gaussian\n",
    "   - If NF4 ≈ FP4: Weights may be heavier-tailed than expected\n",
    "\n",
    "3. **Weight MSE vs Functional MSE:**\n",
    "   - Strong correlation (ρ > 0.8): Weight MSE is a good proxy, uniform importance\n",
    "   - Weak correlation (ρ < 0.5): Weight importance is non-uniform, per-channel schemes critical\n",
    "\n",
    "**Research Opportunities:**\n",
    "- If GPTQ/AWQ gap is large: Implement Hessian-based importance weighting\n",
    "- If NF4 wins significantly: Explore optimal quantization levels for near-Gaussian distributions\n",
    "- If functional MSE deviates: Investigate per-layer and per-channel importance patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
