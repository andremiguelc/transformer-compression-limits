{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c31158d4",
      "metadata": {},
      "source": [
        "# Quantization: Bitsandbytes 4-bit\n",
        "\n",
        "**Objective:** Compare FP16 vs NF4 vs FP4, quantify functional distortion amplification, and relate practical error to Gaussian / GGD bounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5392bad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === AUTHENTICATION (required) ===\n",
        "from huggingface_hub import login\n",
        "# Paste your token directly as a string argument\n",
        "login(token=\"....\")\n",
        "# After running successfully, DELETE this cell or clear the token string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5cc33d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dependency install\n",
        "%pip -q install -U transformers accelerate bitsandbytes scipy\n",
        "# restart after installing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b55235",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from scipy import stats\n",
        "from scipy.stats import gennorm, entropy\n",
        "from scipy.special import gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdfbd69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model + tokenizer (Llama 3.2 1B by default)\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# FP16 baseline\n",
        "m_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ").eval()\n",
        "\n",
        "# Keep the original variable name for downstream cells\n",
        "model = m_fp16\n",
        "print(f\"Loaded: {model_id}\")\n",
        "print(f\"Parameters: {model.num_parameters()/1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3351ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Load bitsandbytes 4-bit Quantized Models\n",
        "# ============================================\n",
        "\n",
        "# FP16 baseline already loaded as `model` (m_fp16)\n",
        "\n",
        "print(\"Loading bnb 4-bit NF4 model...\")\n",
        "try:\n",
        "    q_nf4 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
        "    m_nf4 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=q_nf4,\n",
        "    ).eval()\n",
        "\n",
        "    model_nf4 = m_nf4\n",
        "    NF4_AVAILABLE = True\n",
        "    print(\"✓ NF4 model loaded successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ NF4 model loading failed: {e}\")\n",
        "    print(\"  Install with: pip install bitsandbytes\")\n",
        "    model_nf4 = None\n",
        "    NF4_AVAILABLE = False\n",
        "\n",
        "print(\"\n",
        "Loading bnb 4-bit FP4 model...\")\n",
        "try:\n",
        "    q_fp4 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"fp4\")\n",
        "    m_fp4 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=q_fp4,\n",
        "    ).eval()\n",
        "\n",
        "    model_fp4 = m_fp4\n",
        "    FP4_AVAILABLE = True\n",
        "    print(\"✓ FP4 model loaded successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ FP4 model loading failed: {e}\")\n",
        "    model_fp4 = None\n",
        "    FP4_AVAILABLE = False\n",
        "\n",
        "print(f\"\n",
        "Model availability summary:\")\n",
        "print(f\"  FP16 baseline: ✓ (already loaded)\")\n",
        "print(f\"  bnb 4-bit NF4: {'✓' if NF4_AVAILABLE else '✗'}\")\n",
        "print(f\"  bnb 4-bit FP4: {'✓' if FP4_AVAILABLE else '✗'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e604650c",
      "metadata": {},
      "source": [
        "## 2. Weight MSE Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a446f7af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target layer for comparison\n",
        "target_layer = 8\n",
        "\n",
        "# Extract FP16 baseline weights\n",
        "w_fp16 = model.model.layers[target_layer].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
        "\n",
        "\n",
        "def extract_module_weight(module):\n",
        "    \"\"\"Return dequantized weights for bnb 4-bit modules, or raw weights otherwise.\"\"\"\n",
        "    w = module.weight\n",
        "    if hasattr(w, \"quant_state\"):\n",
        "        try:\n",
        "            import bitsandbytes as bnb\n",
        "            w_deq = bnb.functional.dequantize_4bit(w.data, w.quant_state)\n",
        "            return w_deq.detach().cpu().float().numpy()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if hasattr(w, \"dequantize\"):\n",
        "            return w.dequantize().detach().cpu().float().numpy()\n",
        "    return w.detach().cpu().float().numpy()\n",
        "\n",
        "\n",
        "sigma = float(np.std(w_fp16))\n",
        "d_shannon_gauss = sigma ** 2 / (4 ** 4)  # 4-bit Gaussian bound\n",
        "\n",
        "mse_nf4 = None\n",
        "mse_fp4 = None\n",
        "\n",
        "if NF4_AVAILABLE and model_nf4 is not None:\n",
        "    w_nf4 = extract_module_weight(model_nf4.model.layers[target_layer].mlp.down_proj)\n",
        "    mse_nf4 = float(np.mean((w_fp16 - w_nf4) ** 2))\n",
        "\n",
        "if FP4_AVAILABLE and model_fp4 is not None:\n",
        "    w_fp4 = extract_module_weight(model_fp4.model.layers[target_layer].mlp.down_proj)\n",
        "    mse_fp4 = float(np.mean((w_fp16 - w_fp4) ** 2))\n",
        "\n",
        "rows = []\n",
        "if mse_nf4 is not None:\n",
        "    rows.append({\n",
        "        \"method\": \"bnb 4-bit NF4\",\n",
        "        \"weight_mse\": mse_nf4,\n",
        "        \"gap_bits_vs_gauss\": 0.5 * np.log2(mse_nf4 / d_shannon_gauss),\n",
        "    })\n",
        "if mse_fp4 is not None:\n",
        "    rows.append({\n",
        "        \"method\": \"bnb 4-bit FP4\",\n",
        "        \"weight_mse\": mse_fp4,\n",
        "        \"gap_bits_vs_gauss\": 0.5 * np.log2(mse_fp4 / d_shannon_gauss),\n",
        "    })\n",
        "\n",
        "rows.append({\n",
        "    \"method\": \"Shannon bound (Gaussian)\",\n",
        "    \"weight_mse\": d_shannon_gauss,\n",
        "    \"gap_bits_vs_gauss\": 0.0,\n",
        "})\n",
        "\n",
        "summary_weights = pd.DataFrame(rows)\n",
        "print(summary_weights.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c06495",
      "metadata": {},
      "source": [
        "## 3. Functional Distortion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc6ea7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified calibration data\n",
        "calibration_texts = [\"The quick brown fox jumps over the lazy dog.\"] * 100\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "calibration_inputs = tok(\n",
        "    calibration_texts,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "\n",
        "def iter_batches(inputs, batch_size):\n",
        "    n = inputs[\"input_ids\"].size(0)\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield {k: v[i:i + batch_size] for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "def collect_logits(model, inputs, batch_size=8):\n",
        "    model.eval()\n",
        "    logits_list = []\n",
        "    masks = []\n",
        "    for batch in iter_batches(inputs, batch_size):\n",
        "        device = next(model.parameters()).device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch).logits\n",
        "        logits_list.append(logits.float().cpu())\n",
        "        masks.append(batch[\"attention_mask\"].float().cpu())\n",
        "    return torch.cat(logits_list, dim=0), torch.cat(masks, dim=0)\n",
        "\n",
        "\n",
        "def masked_logits_mse(logits_ref, logits_q, mask):\n",
        "    diff = (logits_ref - logits_q) ** 2\n",
        "    return float((diff * mask.unsqueeze(-1)).sum() / (mask.sum() * diff.shape[-1]))\n",
        "\n",
        "\n",
        "def masked_ce(model, inputs, batch_size=8):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    masks = []\n",
        "    for batch in iter_batches(inputs, batch_size):\n",
        "        device = next(model.parameters()).device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch).logits\n",
        "        logits = logits.float().cpu()\n",
        "        input_ids = batch[\"input_ids\"].cpu()\n",
        "        mask = batch[\"attention_mask\"].cpu()\n",
        "\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = input_ids[:, 1:]\n",
        "        shift_mask = mask[:, 1:]\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            shift_logits.reshape(-1, shift_logits.size(-1)),\n",
        "            shift_labels.reshape(-1),\n",
        "            reduction=\"none\",\n",
        "        ).view(shift_labels.size(0), -1)\n",
        "\n",
        "        losses.append(loss)\n",
        "        masks.append(shift_mask)\n",
        "\n",
        "    loss = torch.cat(losses, dim=0)\n",
        "    mask = torch.cat(masks, dim=0)\n",
        "    return float((loss * mask).sum() / mask.sum())\n",
        "\n",
        "\n",
        "logits_fp16, mask = collect_logits(model, calibration_inputs, batch_size=8)\n",
        "ce_fp16 = masked_ce(model, calibration_inputs, batch_size=8)\n",
        "\n",
        "functional_rows = []\n",
        "\n",
        "if NF4_AVAILABLE and model_nf4 is not None and mse_nf4 is not None:\n",
        "    logits_nf4, _ = collect_logits(model_nf4, calibration_inputs, batch_size=8)\n",
        "    mse_logits = masked_logits_mse(logits_fp16, logits_nf4, mask)\n",
        "    ce_nf4 = masked_ce(model_nf4, calibration_inputs, batch_size=8)\n",
        "    functional_rows.append({\n",
        "        \"method\": \"bnb 4-bit NF4\",\n",
        "        \"weight_mse\": mse_nf4,\n",
        "        \"logits_mse\": mse_logits,\n",
        "        \"delta_ce\": ce_nf4 - ce_fp16,\n",
        "        \"amplification\": mse_logits / mse_nf4,\n",
        "    })\n",
        "\n",
        "if FP4_AVAILABLE and model_fp4 is not None and mse_fp4 is not None:\n",
        "    logits_fp4, _ = collect_logits(model_fp4, calibration_inputs, batch_size=8)\n",
        "    mse_logits = masked_logits_mse(logits_fp16, logits_fp4, mask)\n",
        "    ce_fp4 = masked_ce(model_fp4, calibration_inputs, batch_size=8)\n",
        "    functional_rows.append({\n",
        "        \"method\": \"bnb 4-bit FP4\",\n",
        "        \"weight_mse\": mse_fp4,\n",
        "        \"logits_mse\": mse_logits,\n",
        "        \"delta_ce\": ce_fp4 - ce_fp16,\n",
        "        \"amplification\": mse_logits / mse_fp4,\n",
        "    })\n",
        "\n",
        "functional_summary = pd.DataFrame(functional_rows)\n",
        "print(functional_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54fef365",
      "metadata": {},
      "source": [
        "## 4. Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58b205d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit GGD on a representative layer\n",
        "flat = w_fp16.reshape(-1).astype(np.float32)\n",
        "\n",
        "beta, loc, scale = gennorm.fit(flat)\n",
        "print(f\"GGD shape β = {beta:.2f} (2=Gaussian, 1=Laplacian)\")\n",
        "\n",
        "# One histogram plot\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.hist(flat, bins=100, density=True, alpha=0.5, label=\"Weights\")\n",
        "\n",
        "xs = np.linspace(np.percentile(flat, 0.1), np.percentile(flat, 99.9), 300)\n",
        "plt.plot(xs, gennorm.pdf(xs, beta, loc=loc, scale=scale), \"r-\", label=f\"GGD fit (β={beta:.2f})\")\n",
        "plt.title(f\"Layer {target_layer} Weight Distribution\")\n",
        "plt.xlabel(\"Weight value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mean KL to Gaussian (no per-layer table)\n",
        "\n",
        "def kl_to_gaussian(flat, bins=200):\n",
        "    flat = flat.astype(np.float32)\n",
        "    mu = float(np.mean(flat))\n",
        "    sigma = float(np.std(flat))\n",
        "\n",
        "    hist, bin_edges = np.histogram(flat, bins=bins, density=False)\n",
        "    hist = hist.astype(np.float64)\n",
        "    hist = hist / np.maximum(hist.sum(), 1.0)\n",
        "\n",
        "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
        "    bin_widths = (bin_edges[1:] - bin_edges[:-1])\n",
        "    gaussian_pdf = stats.norm.pdf(bin_centers, mu, sigma)\n",
        "    gaussian_p = gaussian_pdf * bin_widths\n",
        "    gaussian_p = gaussian_p / np.maximum(gaussian_p.sum(), 1.0)\n",
        "\n",
        "    return float(entropy(hist + 1e-12, gaussian_p + 1e-12, base=2))\n",
        "\n",
        "max_samples = 200_000\n",
        "rng = np.random.default_rng(0)\n",
        "kl_vals = []\n",
        "for layer_idx in range(len(model.model.layers)):\n",
        "    w = model.model.layers[layer_idx].mlp.down_proj.weight.detach().cpu().float().numpy()\n",
        "    flat = w.reshape(-1)\n",
        "    if flat.size > max_samples:\n",
        "        flat = rng.choice(flat, size=max_samples, replace=False)\n",
        "    kl_vals.append(kl_to_gaussian(flat, bins=200))\n",
        "\n",
        "mean_kl = float(np.mean(kl_vals))\n",
        "print(f\"Mean KL to Gaussian: {mean_kl:.4f} bits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fd4693",
      "metadata": {},
      "source": [
        "## 5. Gap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff22836",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entropy bounds\n",
        "\n",
        "def ggd_entropy(beta, alpha):\n",
        "    return (1.0 / beta) - np.log2(beta / (2.0 * alpha * gamma(1.0 / beta)))\n",
        "\n",
        "\n",
        "def gaussian_entropy(sigma):\n",
        "    return 0.5 * np.log2(2.0 * np.pi * np.e * sigma ** 2)\n",
        "\n",
        "\n",
        "def distortion_bound_from_entropy(h_bits, rates):\n",
        "    return (2 ** (2 * (h_bits - rates))) / (2 * np.pi * np.e)\n",
        "\n",
        "h_gaussian = gaussian_entropy(sigma)\n",
        "h_ggd = ggd_entropy(beta, scale)\n",
        "bonus = h_gaussian - h_ggd\n",
        "\n",
        "rates = np.linspace(1.0, 8.0, 200)\n",
        "d_gauss = sigma ** 2 / (4 ** rates)\n",
        "d_ggd = distortion_bound_from_entropy(h_ggd, rates)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(rates, d_gauss, label=\"Gaussian bound\", color=\"black\")\n",
        "plt.plot(rates, d_ggd, label=\"GGD bound\", color=\"red\")\n",
        "\n",
        "if mse_nf4 is not None:\n",
        "    plt.scatter([4.0], [mse_nf4], color=\"green\", s=60, label=\"NF4 (4-bit)\")\n",
        "if mse_fp4 is not None:\n",
        "    plt.scatter([4.0], [mse_fp4], color=\"orange\", s=60, label=\"FP4 (4-bit)\")\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Rate (bits/weight)\")\n",
        "plt.ylabel(\"Distortion (MSE)\")\n",
        "plt.title(f\"Layer {target_layer}: R(D) Bound Comparison\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Entropy: Gaussian={h_gaussian:.2f}, GGD={h_ggd:.2f}, Bonus={bonus:.2f} bits\")\n",
        "\n",
        "# Gap quantification at 4 bits\n",
        "bound_ggd_4 = float(distortion_bound_from_entropy(h_ggd, 4.0))\n",
        "print(f\"GGD bound @4 bits: {bound_ggd_4:.2e}\")\n",
        "\n",
        "if mse_nf4 is not None:\n",
        "    gap_bits_nf4 = 0.5 * np.log2(mse_nf4 / bound_ggd_4)\n",
        "    print(f\"NF4 gap from GGD bound: {gap_bits_nf4:.2f} bits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a09192",
      "metadata": {},
      "source": [
        "## 6. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a774040b",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_rows = []\n",
        "\n",
        "if mse_nf4 is not None and not functional_summary.empty:\n",
        "    row = functional_summary[functional_summary[\"method\"] == \"bnb 4-bit NF4\"].iloc[0]\n",
        "    summary_rows.append({\n",
        "        \"method\": \"bnb 4-bit NF4\",\n",
        "        \"weight_mse\": row[\"weight_mse\"],\n",
        "        \"logits_mse\": row[\"logits_mse\"],\n",
        "        \"delta_ce\": row[\"delta_ce\"],\n",
        "        \"amplification\": row[\"amplification\"],\n",
        "        \"gap_bits_vs_gauss\": 0.5 * np.log2(row[\"weight_mse\"] / d_shannon_gauss),\n",
        "        \"gap_bits_vs_ggd\": 0.5 * np.log2(row[\"weight_mse\"] / bound_ggd_4),\n",
        "    })\n",
        "\n",
        "if mse_fp4 is not None and not functional_summary.empty:\n",
        "    row = functional_summary[functional_summary[\"method\"] == \"bnb 4-bit FP4\"].iloc[0]\n",
        "    summary_rows.append({\n",
        "        \"method\": \"bnb 4-bit FP4\",\n",
        "        \"weight_mse\": row[\"weight_mse\"],\n",
        "        \"logits_mse\": row[\"logits_mse\"],\n",
        "        \"delta_ce\": row[\"delta_ce\"],\n",
        "        \"amplification\": row[\"amplification\"],\n",
        "        \"gap_bits_vs_gauss\": 0.5 * np.log2(row[\"weight_mse\"] / d_shannon_gauss),\n",
        "        \"gap_bits_vs_ggd\": 0.5 * np.log2(row[\"weight_mse\"] / bound_ggd_4),\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(summary_df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
